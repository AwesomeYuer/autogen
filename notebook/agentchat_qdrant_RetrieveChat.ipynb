{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc\"></a>\n",
    "# Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering\n",
    "\n",
    "[Qdrant](https://qdrant.tech/) is a high-performance vector search engine/database.\n",
    "\n",
    "This notebook demonstrates the usage of `QdrantRetrieveUserProxyAgent` for RAG, based on [agentchat_RetrieveChat.ipynb](https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb).\n",
    "\n",
    "\n",
    "RetrieveChat is a conversational system for retrieve augmented code generation and question answering. In this notebook, we demonstrate how to utilize RetrieveChat to generate code and answer questions based on customized documentations that are not present in the LLM's training dataset. RetrieveChat uses the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`, which is similar to the usage of `AssistantAgent` and `UserProxyAgent` in other notebooks (e.g., [Automated Task Solving with Code Generation, Execution & Debugging](https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb)).\n",
    "\n",
    "We'll demonstrate usage of RetrieveChat with Qdrant for code generation and question answering w/ human feedback.\n",
    "\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [retrievechat] option.\n",
    "```bash\n",
    "pip install \"pyautogen[retrievechat] flaml[automl] qdrant_client[fastembed]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyautogen[retrievechat] flaml[automl] qdrant_client[fastembed]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-35-turbo-16k-dep-001']\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"OAI_CONFIG_LIST\",\n",
    "    file_location=\".\",\n",
    "    filter_dict={\n",
    "        # \"model\": {\n",
    "        #     \"gpt-4\",\n",
    "        #     \"gpt4\",\n",
    "        #     \"gpt-4-32k\",\n",
    "        #     \"gpt-4-32k-0314\",\n",
    "        #     \"gpt-35-turbo\",\n",
    "        #     \"gpt-3.5-turbo\",\n",
    "        # }\n",
    "        \"model\": [\"gpt-35-turbo-16k-dep-001\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "assert len(config_list) > 0\n",
    "print(\"models to use: \", [config_list[i][\"model\"] for i in range(len(config_list))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 and gpt-3.5-turbo models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted file formats for `docs_path`:\n",
      "['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# Accepted file formats for that can be stored in \n",
    "# a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS\n",
    "\n",
    "print(\"Accepted file formats for `docs_path`:\")\n",
    "print(TEXT_FORMATS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct agents for RetrieveChat\n",
    "\n",
    "We start by initialzing the `RetrieveAssistantAgent` and `QdrantRetrieveUserProxyAgent`. The system message needs to be set to \"You are a helpful assistant.\" for RetrieveAssistantAgent. The detailed instructions are given in the user message. Later we will use the `QdrantRetrieveUserProxyAgent.generate_init_prompt` to combine the instructions and a retrieval augmented generation task for an initial prompt to be sent to the LLM assistant.\n",
    "\n",
    "### You can find the list of all the embedding models supported by Qdrant [here](https://qdrant.github.io/fastembed/examples/Supported_Models/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.qdrant_retrieve_user_proxy_agent import QdrantRetrieveUserProxyAgent\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# 1. create an RetrieveAssistantAgent instance named \"assistant\"\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\", \n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list\n",
    "    },\n",
    ")\n",
    "\n",
    "# 2. create the QdrantRetrieveUserProxyAgent instance named \"ragproxyagent\"\n",
    "# By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "# `docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default, \n",
    "# it is set to None, which works only if the collection is already created.\n",
    "# \n",
    "# Here we generated the documentations from FLAML's docstrings. Not needed if you just want to try this notebook but not to reproduce the\n",
    "# outputs. Clone the FLAML (https://github.com/microsoft/FLAML) repo and navigate to its website folder. Pip install and run `pydoc-markdown`\n",
    "# and it will generate folder `reference` under `website/docs`.\n",
    "#\n",
    "# `task` indicates the kind of task we're working on. In this example, it's a `code` task.\n",
    "# `chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "# We use an in-memory QdrantClient instance here. Not recommended for production.\n",
    "# Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/\n",
    "\n",
    "# add by Awesome Yuer\n",
    "# client = QdrantClient(url=\"http://localhost:6333/\", api_key=\"***\") \n",
    "client = QdrantClient(url=\"http://localhost:6333/\") \n",
    "\n",
    "ragproxyagent = QdrantRetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        # \"docs_path\": \"~/path/to/FLAML/website/docs/reference\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\n",
    "        \"docs_path\": \"../website/docs/reference\",  # change this to your own path, such as https://raw.githubusercontent.com/microsoft/autogen/main/README.md\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \n",
    "        # add by Awesome Yuer\n",
    "        # \"client\": QdrantClient(\":memory:\"),\n",
    "        \n",
    "        \"client\": client,\n",
    "\n",
    "        # Awesome Yuer\n",
    "        \"embedding_model\": \"BAAI/bge-small-en-v1.5\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add by Awesome Yuer\n",
    "\n",
    "# We use an in-memory QdrantClient instance here. Not recommended for production.\n",
    "# Get the installation instructions here: https://qdrant.tech/documentation/guides/installation/\n",
    "\n",
    "# 临时切换环境生成 reference\n",
    "#! conda activate misc-py311-env\n",
    "\n",
    "! python -m pip install pydoc-markdown\n",
    "\n",
    "! cd website\n",
    "\n",
    "! pydoc-markdown\n",
    "\n",
    "#! conda activate autogen-py311-env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-1\"></a>\n",
    "### Example 1\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question and ask for human-in-loop feedbacks.\n",
    "\n",
    "Problem: Is there a function named `tune_automl` in FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77.7M/77.7M [00:08<00:00, 9.67MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding doc_id 0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 13 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 16 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 15 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 12 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 17 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 18 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 21 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 6 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 11 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 7 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Is there a function called tune_automl?\n",
      "\n",
      "Context is: {\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"items\": [\n",
      "            \"reference/agentchat/contrib/math_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_assistant_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/teachable_agent\",\n",
      "            \"reference/agentchat/contrib/text_analyzer_agent\"\n",
      "          ],\n",
      "          \"label\": \"agentchat.contrib\",\n",
      "          \"type\": \"category\"\n",
      "        },\n",
      "        \"reference/agentchat/agent\",\n",
      "        \"reference/agentchat/assistant_agent\",\n",
      "        \"reference/agentchat/conversable_agent\",\n",
      "        \"reference/agentchat/groupchat\",\n",
      "        \"reference/agentchat/user_proxy_agent\"\n",
      "      ],\n",
      "      \"label\": \"agentchat\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    {\n",
      "      \"items\": [\n",
      "        \"reference/oai/completion\",\n",
      "        \"reference/oai/openai_utils\"\n",
      "      ],\n",
      "      \"label\": \"oai\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    \"reference/code_utils\",\n",
      "    \"reference/math_utils\",\n",
      "    \"reference/retrieve_utils\",\n",
      "    \"reference/token_count_utils\"\n",
      "  ],\n",
      "  \"label\": \"Reference\",\n",
      "  \"type\": \"category\"\n",
      "}\n",
      "---\n",
      "sidebar_label: qdrant_retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.qdrant_retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## QdrantRetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             retrieve_config: Optional[Dict] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models\n",
      "  can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n",
      "  Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "  - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.\n",
      "  - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n",
      "  - quantization_config: Quantization configuration. If None, quantization will be disabled.\n",
      "  - hnsw_config: HNSW configuration. If None, default configuration will be used.\n",
      "  You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/`vector`-index.\n",
      "  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n",
      "  - payload_indexing: Whether to create a payload index for the document field. Default is False.\n",
      "  You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/`payload`-index\n",
      "  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### create\\_qdrant\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def create_qdrant_from_dir(\n",
      "        dir_path: str,\n",
      "        max_tokens: int = 4000,\n",
      "        client: QdrantClient = None,\n",
      "        collection_name: str = \"all-my-documents\",\n",
      "        chunk_mode: str = \"multi_lines\",\n",
      "        must_break_at_empty_line: bool = True,\n",
      "        embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n",
      "        custom_text_split_function: Callable = None,\n",
      "        parallel: int = 0,\n",
      "        on_disk: bool = False,\n",
      "        quantization_config: Optional[models.QuantizationConfig] = None,\n",
      "        hnsw_config: Optional[models.HnswConfigDiff] = None,\n",
      "        payload_indexing: bool = False,\n",
      "        qdrant_client_options: Optional[Dict] = {})\n",
      "```\n",
      "\n",
      "Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a url to\n",
      "a single file.\n",
      "\n",
      "**Arguments**:\n",
      "---\n",
      "sidebar_label: retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## RetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             retrieve_config: Optional[Dict] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client `chromadb.Client()`\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\n",
      "  can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\n",
      "  fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\n",
      "  - embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None,\n",
      "  SentenceTransformer with the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or\n",
      "  other embedding functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - get_or_create (Optional, bool): if True, will create/recreate a collection for the retrieve chat.\n",
      "  This is the same as that used in chromadb. Default is False. Will be set to False if docs_path is None.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take (text:str, model:str) as input and return the token_count(int). the retrieve_config[\"model\"] will be passed in the function.\n",
      "  Default is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "  \n",
      "  Example of overriding retrieve_docs:\n",
      "  If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code.\n",
      "```python\n",
      "class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n",
      "    def query_vector_db(\n",
      "        self,\n",
      "        query_texts: List[str],\n",
      "        n_results: int = 10,\n",
      "        search_string: str = \"\",\n",
      "        **kwargs,\n",
      "    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
      "        # define your own query function here\n",
      "        pass\n",
      "\n",
      "    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
      "        results = self.query_vector_db(\n",
      "            query_texts=[problem],\n",
      "            n_results=n_results,\n",
      "            search_string=search_string,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        self._results = results\n",
      "        print(\"doc_ids: \", results[\"ids\"])\n",
      "```\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Retrieve docs based on the given problem and assign the results to the class property `_results`.\n",
      "In case you want to customize the retrieval process, such as using a different vector db whose APIs are not\n",
      "compatible with chromadb or filter results with metadata, you can override this function. Just keep the current\n",
      "parameters and add your own parameters with default values, and keep the results in below type.\n",
      "\n",
      "Type of the results: Dict[str, List[List[Any]]], should have keys \"ids\" and \"documents\", \"ids\" for the ids of\n",
      "the retrieved docs and \"documents\" for the contents of the retrieved docs. Any other keys are optional. Refer\n",
      "to `chromadb.api.types.QueryResult` as an example.\n",
      "ids: List[string]\n",
      "documents: List[List[string]]\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem: str,\n",
      "                          n_results: int = 20,\n",
      "                          search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Generate an initial message with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: retrieve_assistant_agent\n",
      "title: agentchat.contrib.retrieve_assistant_agent\n",
      "---\n",
      "\n",
      "## RetrieveAssistantAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveAssistantAgent(AssistantAgent)\n",
      "```\n",
      "\n",
      "(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.\n",
      "\n",
      "RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.\n",
      "The default system message is designed to solve a task with LLM,\n",
      "including suggesting python code blocks and debugging.\n",
      "`human_input_mode` is default to \"NEVER\"\n",
      "and `code_execution_config` is default to False.\n",
      "This agent doesn't execute code by default, and expects the user to execute the code.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: math_user_proxy_agent\n",
      "title: agentchat.contrib.math_user_proxy_agent\n",
      "---\n",
      "\n",
      "## MathUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class MathUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "(Experimental) A MathChat agent that can handle math problems.\n",
      "\n",
      "#### MAX\\_CONSECUTIVE\\_AUTO\\_REPLY\n",
      "\n",
      "maximum number of consecutive auto replies (subject to future change)\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: Optional[str] = \"MathChatAgent\",\n",
      "             is_termination_msg: Optional[Callable[\n",
      "                 [Dict], bool]] = _is_termination_msg_mathchat,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             default_auto_reply: Optional[Union[str, Dict,\n",
      "                                                None]] = DEFAULT_REPLY,\n",
      "             max_invalid_q_per_step=3,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) (Default) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `default_auto_reply` _str or dict or None_ - the default auto reply message when no code execution or llm based reply is generated.\n",
      "- `max_invalid_q_per_step` _int_ - (ADDED) the maximum number of invalid queries per step.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem,\n",
      "                          prompt_type=\"default\",\n",
      "                          customized_prompt=None)\n",
      "```\n",
      "\n",
      "Generate a prompt for the assistant agent with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `prompt_type` _str_ - the type of the prompt. Possible values are \"default\", \"python\", \"wolfram\".\n",
      "  (1) \"default\": the prompt that allows the agent to choose between 3 ways to solve a problem:\n",
      "  1. write a python program to solve it directly.\n",
      "  2. solve it directly without python.\n",
      "  3. solve it step by step with python.\n",
      "  (2) \"python\":\n",
      "  a simplified prompt from the third way of the \"default\" prompt, that asks the assistant\n",
      "  to solve the problem step by step with python.\n",
      "  (3) \"two_tools\":\n",
      "  a simplified prompt similar to the \"python\" prompt, but allows the model to choose between\n",
      "  Python and Wolfram Alpha to solve the problem.\n",
      "- `customized_prompt` _str_ - a customized prompt to be used. If it is not None, the prompt_type will be ignored.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "#### execute\\_one\\_python\\_code\n",
      "\n",
      "```python\n",
      "def execute_one_python_code(pycode)\n",
      "```\n",
      "\n",
      "Execute python code blocks.\n",
      "\n",
      "Previous python code will be saved and executed together with the new code.\n",
      "the \"print\" function will also be added to the last line of the code if needed\n",
      "\n",
      "#### execute\\_one\\_wolfram\\_query\n",
      "\n",
      "```python\n",
      "def execute_one_wolfram_query(query: str)\n",
      "```\n",
      "\n",
      "Run one wolfram query and return the output.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query` - string of the query.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `output` - string with the output of the query.\n",
      "- `is_success` - boolean indicating whether the query was successful.\n",
      "\n",
      "#### get\\_from\\_dict\\_or\\_env\n",
      "\n",
      "```python\n",
      "def get_from_dict_or_env(data: Dict[str, Any],\n",
      "                         key: str,\n",
      "                         env_key: str,\n",
      "                         default: Optional[str] = None) -> str\n",
      "```\n",
      "\n",
      "Get a value from a dictionary or an environment variable.\n",
      "\n",
      "## WolframAlphaAPIWrapper Objects\n",
      "\n",
      "```python\n",
      "class WolframAlphaAPIWrapper(BaseModel)\n",
      "```\n",
      "\n",
      "Wrapper for Wolfram Alpha.\n",
      "\n",
      "Docs for using:\n",
      "\n",
      "1. Go to wolfram alpha and sign up for a developer account\n",
      "2. Create an app and get your APP ID\n",
      "3. Save your APP ID into WOLFRAM_ALPHA_APPID env variable\n",
      "4. pip install wolframalpha\n",
      "\n",
      "#### wolfram\\_client\n",
      "\n",
      ":meta private:\n",
      "\n",
      "## Config Objects\n",
      "\n",
      "```python\n",
      "class Config()\n",
      "```\n",
      "\n",
      "Configuration for this pydantic object.\n",
      "\n",
      "#### validate\\_environment\n",
      "\n",
      "```python\n",
      "@root_validator(skip_on_failure=True)\n",
      "def validate_environment(cls, values: Dict) -> Dict\n",
      "```\n",
      "\n",
      "Validate that api key and python package exists in environment.\n",
      "\n",
      "#### run\n",
      "\n",
      "```python\n",
      "def run(query: str) -> str\n",
      "```\n",
      "\n",
      "Run query through WolframAlpha and parse result.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: teachable_agent\n",
      "title: agentchat.contrib.teachable_agent\n",
      "---\n",
      "\n",
      "## TeachableAgent Objects\n",
      "\n",
      "```python\n",
      "class TeachableAgent(ConversableAgent)\n",
      "```\n",
      "\n",
      "Teachable Agent, a subclass of ConversableAgent using a vector database to remember user teachings.\n",
      "In this class, the term 'user' refers to any caller (human or not) sending messages to this agent.\n",
      "Not yet tested in the group-chat setting.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(\n",
      "        name=\"teachableagent\",\n",
      "        system_message:\n",
      "    Optional[\n",
      "        str] = \"You are a helpful AI assistant that remembers user teachings from prior chats.\",\n",
      "        human_input_mode: Optional[str] = \"NEVER\",\n",
      "        llm_config: Optional[Union[Dict, bool]] = None,\n",
      "        analyzer_llm_config: Optional[Union[Dict, bool]] = None,\n",
      "        teach_config: Optional[Dict] = None,\n",
      "        **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "- `human_input_mode` _str_ - This agent should NEVER prompt the human for input.\n",
      "- `llm_config` _dict or False_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "  To disable llm-based auto reply, set to False.\n",
      "- `analyzer_llm_config` _dict or False_ - llm inference configuration passed to TextAnalyzerAgent.\n",
      "  Given the default setting of None, TeachableAgent passes its own llm_config to TextAnalyzerAgent.\n",
      "- `teach_config` _dict or None_ - Additional parameters used by TeachableAgent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with any of the following keys:\n",
      "  - verbosity (Optional, int): # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
      "  - reset_db (Optional, bool): True to clear the DB before starting. Default False.\n",
      "  - path_to_db_dir (Optional, str): path to the directory where the DB is stored. Default \"./tmp/teachable_agent_db\"\n",
      "  - prepopulate (Optional, int): True (default) to prepopulate the DB with a set of input-output pairs.\n",
      "  - recall_threshold (Optional, float): The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.\n",
      "  - max_num_retrievals (Optional, int): The maximum number of memos to retrieve from the DB. Default 10.\n",
      "- `**kwargs` _dict_ - other kwargs in [ConversableAgent](../conversable_agent#__init__).\n",
      "\n",
      "#### close\\_db\n",
      "\n",
      "```python\n",
      "def close_db()\n",
      "```\n",
      "\n",
      "Cleanly closes the memo store.\n",
      "\n",
      "#### prepopulate\\_db\n",
      "\n",
      "```python\n",
      "def prepopulate_db()\n",
      "```\n",
      "\n",
      "Adds a few arbitrary memos to the DB.\n",
      "\n",
      "#### learn\\_from\\_user\\_feedback\n",
      "\n",
      "```python\n",
      "def learn_from_user_feedback()\n",
      "```\n",
      "\n",
      "Reviews the user comments from the last chat, and decides what teachings to store as memos.\n",
      "\n",
      "#### consider\\_memo\\_storage\n",
      "\n",
      "```python\n",
      "def consider_memo_storage(comment)\n",
      "```\n",
      "\n",
      "Decides whether to store something from one user comment in the DB.\n",
      "\n",
      "#### consider\\_memo\\_retrieval\n",
      "\n",
      "```python\n",
      "def consider_memo_retrieval(comment)\n",
      "```\n",
      "\n",
      "Decides whether to retrieve memos from the DB, and add them to the chat context.\n",
      "\n",
      "#### retrieve\\_relevant\\_memos\n",
      "\n",
      "```python\n",
      "def retrieve_relevant_memos(input_text)\n",
      "```\n",
      "\n",
      "Returns semantically related memos from the DB.\n",
      "\n",
      "#### concatenate\\_memo\\_texts\n",
      "\n",
      "```python\n",
      "def concatenate_memo_texts(memo_list)\n",
      "```\n",
      "\n",
      "Concatenates the memo texts into a single string for inclusion in the chat context.\n",
      "\n",
      "#### analyze\n",
      "\n",
      "```python\n",
      "def analyze(text_to_analyze, analysis_instructions)\n",
      "```\n",
      "\n",
      "Asks TextAnalyzerAgent to analyze the given text according to specific instructions.\n",
      "\n",
      "## MemoStore Objects\n",
      "\n",
      "```python\n",
      "class MemoStore()\n",
      "```\n",
      "\n",
      "Provides memory storage and retrieval for a TeachableAgent, using a vector database.\n",
      "Each DB entry (called a memo) is a pair of strings: an input text and an output text.\n",
      "The input text might be a question, or a task to perform.\n",
      "The output text might be an answer to the question, or advice on how to perform the task.\n",
      "Vector embeddings are currently supplied by Chroma's default Sentence Transformers.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(verbosity, reset, path_to_db_dir)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "  - verbosity (Optional, int): 1 to print memory operations, 0 to omit them. 3+ to print memo lists.\n",
      "  - path_to_db_dir (Optional, str): path to the directory where the DB is stored.\n",
      "\n",
      "#### list\\_memos\n",
      "\n",
      "```python\n",
      "def list_memos()\n",
      "```\n",
      "\n",
      "Prints the contents of MemoStore.\n",
      "\n",
      "#### close\n",
      "\n",
      "```python\n",
      "def close()\n",
      "```\n",
      "\n",
      "Saves self.uid_text_dict to disk.\n",
      "\n",
      "#### reset\\_db\n",
      "\n",
      "```python\n",
      "def reset_db()\n",
      "```\n",
      "\n",
      "Forces immediate deletion of the DB's contents, in memory and on disk.\n",
      "\n",
      "#### add\\_input\\_output\\_pair\n",
      "\n",
      "```python\n",
      "def add_input_output_pair(input_text, output_text)\n",
      "```\n",
      "\n",
      "Adds an input-output pair to the vector DB.\n",
      "\n",
      "#### get\\_nearest\\_memo\n",
      "\n",
      "```python\n",
      "def get_nearest_memo(query_text)\n",
      "```\n",
      "\n",
      "Retrieves the nearest memo to the given query text.\n",
      "\n",
      "#### get\\_related\\_memos\n",
      "\n",
      "```python\n",
      "def get_related_memos(query_text, n_results, threshold)\n",
      "```\n",
      "\n",
      "Retrieves memos that are related to the given query text within the specified distance threshold.\n",
      "\n",
      "#### prepopulate\n",
      "\n",
      "```python\n",
      "def prepopulate()\n",
      "```\n",
      "\n",
      "Adds a few arbitrary examples to the vector DB, just to make retrieval less trivial.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: text_analyzer_agent\n",
      "title: agentchat.contrib.text_analyzer_agent\n",
      "---\n",
      "\n",
      "## TextAnalyzerAgent Objects\n",
      "\n",
      "```python\n",
      "class TextAnalyzerAgent(ConversableAgent)\n",
      "```\n",
      "\n",
      "Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"analyzer\",\n",
      "             system_message: Optional[str] = system_message,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             llm_config: Optional[Union[Dict, bool]] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "- `human_input_mode` _str_ - This agent should NEVER prompt the human for input.\n",
      "- `llm_config` _dict or False_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "  To disable llm-based auto reply, set to False.\n",
      "- `teach_config` _dict or None_ - Additional parameters used by TeachableAgent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with any of the following keys:\n",
      "  - verbosity (Optional, int): # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
      "  - reset_db (Optional, bool): True to clear the DB before starting. Default False.\n",
      "  - path_to_db_dir (Optional, str): path to the directory where the DB is stored. Default \"./tmp/teachable_agent_db\"\n",
      "  - prepopulate (Optional, int): True (default) to prepopulate the DB with a set of input-output pairs.\n",
      "  - recall_threshold (Optional, float): The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.\n",
      "  - max_num_retrievals (Optional, int): The maximum number of memos to retrieve from the DB. Default 10.\n",
      "- `**kwargs` _dict_ - other kwargs in [ConversableAgent](../conversable_agent#__init__).\n",
      "\n",
      "#### analyze\\_text\n",
      "\n",
      "```python\n",
      "def analyze_text(text_to_analyze, analysis_instructions)\n",
      "```\n",
      "\n",
      "Analyzes the given text as instructed, and returns the analysis.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: openai_utils\n",
      "title: oai.openai_utils\n",
      "---\n",
      "\n",
      "#### get\\_key\n",
      "\n",
      "```python\n",
      "def get_key(config)\n",
      "```\n",
      "\n",
      "Get a unique identifier of a configuration.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `config` _dict or list_ - A configuration.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `tuple` - A unique identifier which can be used as a key for a dict.\n",
      "\n",
      "#### get\\_config\\_list\n",
      "\n",
      "```python\n",
      "def get_config_list(api_keys: List,\n",
      "                    api_bases: Optional[List] = None,\n",
      "                    api_type: Optional[str] = None,\n",
      "                    api_version: Optional[str] = None) -> List[Dict]\n",
      "```\n",
      "\n",
      "Get a list of configs for openai api calls.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `api_keys` _list_ - The api keys for openai api calls.\n",
      "- `api_bases` _list, optional_ - The api bases for openai api calls.\n",
      "- `api_type` _str, optional_ - The api type for openai api calls.\n",
      "- `api_version` _str, optional_ - The api version for openai api calls.\n",
      "\n",
      "#### config\\_list\\_openai\\_aoai\n",
      "\n",
      "```python\n",
      "def config_list_openai_aoai(\n",
      "        key_file_path: Optional[str] = \".\",\n",
      "        openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
      "        aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
      "        aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
      "        exclude: Optional[str] = None) -> List[Dict]\n",
      "```\n",
      "\n",
      "Get a list of configs for openai + azure openai api calls.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `key_file_path` _str, optional_ - The path to the key files.\n",
      "- `openai_api_key_file` _str, optional_ - The file name of the openai api key.\n",
      "- `aoai_api_key_file` _str, optional_ - The file name of the azure openai api key.\n",
      "- `aoai_api_base_file` _str, optional_ - The file name of the azure openai api base.\n",
      "- `exclude` _str, optional_ - The api type to exclude, \"openai\" or \"aoai\".\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - A list of configs for openai api calls.\n",
      "\n",
      "#### config\\_list\\_from\\_models\n",
      "\n",
      "```python\n",
      "def config_list_from_models(\n",
      "        key_file_path: Optional[str] = \".\",\n",
      "        openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
      "        aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
      "        aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
      "        exclude: Optional[str] = None,\n",
      "        model_list: Optional[list] = None) -> List[Dict]\n",
      "```\n",
      "\n",
      "Get a list of configs for api calls with models in the model list.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `key_file_path` _str, optional_ - The path to the key files.\n",
      "- `openai_api_key_file` _str, optional_ - The file name of the openai api key.\n",
      "- `aoai_api_key_file` _str, optional_ - The file name of the azure openai api key.\n",
      "- `aoai_api_base_file` _str, optional_ - The file name of the azure openai api base.\n",
      "- `exclude` _str, optional_ - The api type to exclude, \"openai\" or \"aoai\".\n",
      "- `model_list` _list, optional_ - The model list.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - A list of configs for openai api calls.\n",
      "\n",
      "#### config\\_list\\_gpt4\\_gpt35\n",
      "\n",
      "```python\n",
      "def config_list_gpt4_gpt35(\n",
      "        key_file_path: Optional[str] = \".\",\n",
      "        openai_api_key_file: Optional[str] = \"key_openai.txt\",\n",
      "        aoai_api_key_file: Optional[str] = \"key_aoai.txt\",\n",
      "        aoai_api_base_file: Optional[str] = \"base_aoai.txt\",\n",
      "        exclude: Optional[str] = None) -> List[Dict]\n",
      "```\n",
      "\n",
      "Get a list of configs for gpt-4 followed by gpt-3.5 api calls.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `key_file_path` _str, optional_ - The path to the key files.\n",
      "- `openai_api_key_file` _str, optional_ - The file name of the openai api key.\n",
      "- `aoai_api_key_file` _str, optional_ - The file name of the azure openai api key.\n",
      "- `aoai_api_base_file` _str, optional_ - The file name of the azure openai api base.\n",
      "- `exclude` _str, optional_ - The api type to exclude, \"openai\" or \"aoai\".\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - A list of configs for openai api calls.\n",
      "\n",
      "#### filter\\_config\n",
      "\n",
      "```python\n",
      "def filter_config(config_list, filter_dict)\n",
      "```\n",
      "\n",
      "Filter the config list by provider and model.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `config_list` _list_ - The config list.\n",
      "- `filter_dict` _dict, optional_ - The filter dict with keys corresponding to a field in each config,\n",
      "  and values corresponding to lists of acceptable values for each key.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - The filtered config list.\n",
      "\n",
      "#### config\\_list\\_from\\_json\n",
      "\n",
      "```python\n",
      "def config_list_from_json(\n",
      "    env_or_file: str,\n",
      "    file_location: Optional[str] = \"\",\n",
      "    filter_dict: Optional[Dict[str, Union[List[Union[str, None]],\n",
      "                                          Set[Union[str, None]]]]] = None\n",
      ") -> List[Dict]\n",
      "```\n",
      "\n",
      "Get a list of configs from a json parsed from an env variable or a file.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `env_or_file` _str_ - The env variable name or file name.\n",
      "- `file_location` _str, optional_ - The file location.\n",
      "- `filter_dict` _dict, optional_ - The filter dict with keys corresponding to a field in each config,\n",
      "  and values corresponding to lists of acceptable values for each key.\n",
      "  e.g.,\n",
      "```python\n",
      "filter_dict = {\n",
      "    \"api_type\": [\"open_ai\", None],  # None means a missing key is acceptable\n",
      "    \"model\": [\"gpt-3.5-turbo\", \"gpt-4\"],\n",
      "}\n",
      "```\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - A list of configs for openai api calls.\n",
      "\n",
      "#### get\\_config\n",
      "\n",
      "```python\n",
      "def get_config(api_key: str,\n",
      "               api_base: Optional[str] = None,\n",
      "               api_type: Optional[str] = None,\n",
      "               api_version: Optional[str] = None) -> Dict\n",
      "```\n",
      "\n",
      "Construct a configuration dictionary with the provided API configurations.\n",
      "Appending the additional configurations to the config only if they're set\n",
      "\n",
      "example:\n",
      ">> model_api_key_map={\n",
      "\"gpt-4\": \"OPENAI_API_KEY\",\n",
      "\"gpt-3.5-turbo\": {\n",
      "\"api_key_env_var\": \"ANOTHER_API_KEY\",\n",
      "\"api_type\": \"aoai\",\n",
      "\"api_version\": \"v2\",\n",
      "\"api_base\": \"https://api.someotherapi.com\"\n",
      "}\n",
      "}\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `api_key` _str_ - The API key used for authenticating API requests.\n",
      "- `api_base` _str, optional_ - The base URL of the API. Defaults to None.\n",
      "- `api_type` _str, optional_ - The type or kind of API. Defaults to None.\n",
      "- `api_version` _str, optional_ - The API version. Defaults to None.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `Dict` - A dictionary containing the API configurations.\n",
      "\n",
      "#### config\\_list\\_from\\_dotenv\n",
      "\n",
      "```python\n",
      "def config_list_from_dotenv(\n",
      "    dotenv_file_path: Optional[str] = None,\n",
      "    model_api_key_map: Optional[dict] = None,\n",
      "    filter_dict: Optional[dict] = None\n",
      ") -> List[Dict[str, Union[str, Set[str]]]]\n",
      "```\n",
      "\n",
      "Load API configurations from a specified .env file or environment variables and construct a list of configurations.\n",
      "\n",
      "This function will:\n",
      "- Load API keys from a provided .env file or from existing environment variables.\n",
      "- Create a configuration dictionary for each model using the API keys and additional configurations.\n",
      "- Filter and return the configurations based on provided filters.\n",
      "\n",
      "model_api_key_map will default to `{\"gpt-4\": \"OPENAI_API_KEY\", \"gpt-3.5-turbo\": \"OPENAI_API_KEY\"}` if none\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `dotenv_file_path` _str, optional_ - The path to the .env file. Defaults to None.\n",
      "- `model_api_key_map` _str/dict, optional_ - A dictionary mapping models to their API key configurations.\n",
      "  If a string is provided as configuration, it is considered as an environment\n",
      "  variable name storing the API key.\n",
      "  If a dict is provided, it should contain at least 'api_key_env_var' key,\n",
      "  and optionally other API configurations like 'api_base', 'api_type', and 'api_version'.\n",
      "  Defaults to a basic map with 'gpt-4' and 'gpt-3.5-turbo' mapped to 'OPENAI_API_KEY'.\n",
      "- `filter_dict` _dict, optional_ - A dictionary containing the models to be loaded.\n",
      "  Containing a 'model' key mapped to a set of model names to be loaded.\n",
      "  Defaults to None, which loads all found configurations.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  List[Dict[str, Union[str, Set[str]]]]: A list of configuration dictionaries for each model.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "---\n",
      "sidebar_label: assistant_agent\n",
      "title: agentchat.assistant_agent\n",
      "---\n",
      "\n",
      "## AssistantAgent Objects\n",
      "\n",
      "```python\n",
      "class AssistantAgent(ConversableAgent)\n",
      "```\n",
      "\n",
      "(In preview) Assistant agent, designed to solve a task with LLM.\n",
      "\n",
      "AssistantAgent is a subclass of ConversableAgent configured with a default system message.\n",
      "The default system message is designed to solve a task with LLM,\n",
      "including suggesting python code blocks and debugging.\n",
      "`human_input_mode` is default to \"NEVER\"\n",
      "and `code_execution_config` is default to False.\n",
      "This agent doesn't execute code by default, and expects the user to execute the code.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str,\n",
      "             system_message: Optional[str] = DEFAULT_SYSTEM_MESSAGE,\n",
      "             llm_config: Optional[Union[Dict, bool]] = None,\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             max_consecutive_auto_reply: Optional[int] = None,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             code_execution_config: Optional[Union[Dict, bool]] = False,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - agent name.\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "  Please override this attribute if you want to reprogram the agent.\n",
      "- `llm_config` _dict_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "- `**kwargs` _dict_ - Please refer to other kwargs in\n",
      "  [ConversableAgent](conversable_agent#__init__).\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: user_proxy_agent\n",
      "title: agentchat.user_proxy_agent\n",
      "---\n",
      "\n",
      "## UserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class UserProxyAgent(ConversableAgent)\n",
      "```\n",
      "\n",
      "(In preview) A proxy agent for the user, that can execute code and provide feedback to the other agents.\n",
      "\n",
      "UserProxyAgent is a subclass of ConversableAgent configured with `human_input_mode` to ALWAYS\n",
      "and `llm_config` to False. By default, the agent will prompt for human input every time a message is received.\n",
      "Code execution is enabled by default. LLM-based auto reply is disabled by default.\n",
      "To modify auto reply, register a method with [`register_reply`](conversable_agent#register_reply).\n",
      "To modify the way to get human input, override `get_human_input` method.\n",
      "To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,\n",
      "`run_code`, and `execute_function` methods respectively.\n",
      "To customize the initial message when a conversation starts, override `generate_init_message` method.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str,\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             max_consecutive_auto_reply: Optional[int] = None,\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             function_map: Optional[Dict[str, Callable]] = None,\n",
      "             code_execution_config: Optional[Union[Dict, bool]] = None,\n",
      "             default_auto_reply: Optional[Union[str, Dict, None]] = \"\",\n",
      "             llm_config: Optional[Union[Dict, bool]] = False,\n",
      "             system_message: Optional[str] = \"\")\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  The limit only plays a role when human_input_mode is not \"ALWAYS\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `function_map` _dict[str, callable]_ - Mapping function names (passed to openai) to callable functions.\n",
      "- `code_execution_config` _dict or False_ - config for the code execution.\n",
      "  To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
      "  - work_dir (Optional, str): The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_autogen\".\n",
      "  - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is True, which will be converted into a list.\n",
      "  If the code is executed in the current environment,\n",
      "  the code must be trusted.\n",
      "  - timeout (Optional, int): The maximum execution time in seconds.\n",
      "  - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\n",
      "- `default_auto_reply` _str or dict or None_ - the default auto reply message when no code execution or llm based reply is generated.\n",
      "- `llm_config` _dict or False_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "  Default to false, which disables llm-based auto reply.\n",
      "- `system_message` _str_ - system message for ChatCompletion inference.\n",
      "  Only used when llm_config is not False. Use it to reprogram the agent.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: conversable_agent\n",
      "title: agentchat.conversable_agent\n",
      "---\n",
      "\n",
      "## ConversableAgent Objects\n",
      "\n",
      "```python\n",
      "class ConversableAgent(Agent)\n",
      "```\n",
      "\n",
      "(In preview) A class for generic conversable agents which can be configured as assistant or user proxy.\n",
      "\n",
      "After receiving each message, the agent will send a reply to the sender unless the msg is a termination msg.\n",
      "For example, AssistantAgent and UserProxyAgent are subclasses of this class,\n",
      "configured with different default settings.\n",
      "\n",
      "To modify auto reply, override `generate_reply` method.\n",
      "To disable/enable human response in every turn, set `human_input_mode` to \"NEVER\" or \"ALWAYS\".\n",
      "To modify the way to get human input, override `get_human_input` method.\n",
      "To modify the way to execute code blocks, single code block, or function call, override `execute_code_blocks`,\n",
      "`run_code`, and `execute_function` methods respectively.\n",
      "To customize the initial message when a conversation starts, override `generate_init_message` method.\n",
      "\n",
      "#### MAX\\_CONSECUTIVE\\_AUTO\\_REPLY\n",
      "\n",
      "maximum number of consecutive auto replies (subject to future change)\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: str,\n",
      "             system_message: Optional[str] = \"You are a helpful AI Assistant.\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             max_consecutive_auto_reply: Optional[int] = None,\n",
      "             human_input_mode: Optional[str] = \"TERMINATE\",\n",
      "             function_map: Optional[Dict[str, Callable]] = None,\n",
      "             code_execution_config: Optional[Union[Dict, bool]] = None,\n",
      "             llm_config: Optional[Union[Dict, bool]] = None,\n",
      "             default_auto_reply: Optional[Union[str, Dict, None]] = \"\")\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `max_consecutive_auto_reply` _int_ - the maximum number of consecutive auto replies.\n",
      "  default to None (no limit provided, class attribute MAX_CONSECUTIVE_AUTO_REPLY will be used as the limit in this case).\n",
      "  When set to 0, no auto reply will be generated.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `function_map` _dict[str, callable]_ - Mapping function names (passed to openai) to callable functions.\n",
      "- `code_execution_config` _dict or False_ - config for the code execution.\n",
      "  To disable code execution, set to False. Otherwise, set to a dictionary with the following keys:\n",
      "  - work_dir (Optional, str): The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_autogen\".\n",
      "  - use_docker (Optional, list, str or bool): The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is True when the docker python package is installed.\n",
      "  When set to True, a default list will be used.\n",
      "  We strongly recommend using docker for code execution.\n",
      "  - timeout (Optional, int): The maximum execution time in seconds.\n",
      "  - last_n_messages (Experimental, Optional, int): The number of messages to look back for code execution. Default to 1.\n",
      "- `llm_config` _dict or False_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "  To disable llm-based auto reply, set to False.\n",
      "- `default_auto_reply` _str or dict or None_ - default auto reply when no code execution or llm-based reply is generated.\n",
      "\n",
      "#### register\\_reply\n",
      "\n",
      "```python\n",
      "def register_reply(trigger: Union[Type[Agent], str, Agent,\n",
      "                                  Callable[[Agent], bool], List],\n",
      "                   reply_func: Callable,\n",
      "                   position: Optional[int] = 0,\n",
      "                   config: Optional[Any] = None,\n",
      "                   reset_config: Optional[Callable] = None)\n",
      "```\n",
      "\n",
      "Register a reply function.\n",
      "\n",
      "The reply function will be called when the trigger matches the sender.\n",
      "The function registered later will be checked earlier by default.\n",
      "To change the order, set the position to a positive integer.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `trigger` _Agent class, str, Agent instance, callable, or list_ - the trigger.\n",
      "  - If a class is provided, the reply function will be called when the sender is an instance of the class.\n",
      "  - If a string is provided, the reply function will be called when the sender's name matches the string.\n",
      "  - If an agent instance is provided, the reply function will be called when the sender is the agent instance.\n",
      "  - If a callable is provided, the reply function will be called when the callable returns True.\n",
      "  - If a list is provided, the reply function will be called when any of the triggers in the list is activated.\n",
      "  - If None is provided, the reply function will be called only when the sender is None.\n",
      "- `Note` - Be sure to register `None` as a trigger if you would like to trigger an auto-reply function with non-empty messages and `sender=None`.\n",
      "- `reply_func` _Callable_ - the reply function.\n",
      "  The function takes a recipient agent, a list of messages, a sender agent and a config as input and returns a reply message.\n",
      "```python\n",
      "def reply_func(\n",
      "    recipient: ConversableAgent,\n",
      "    messages: Optional[List[Dict]] = None,\n",
      "    sender: Optional[Agent] = None,\n",
      "    config: Optional[Any] = None,\n",
      ") -> Union[str, Dict, None]:\n",
      "```\n",
      "- `position` _int_ - the position of the reply function in the reply function list.\n",
      "  The function registered later will be checked earlier by default.\n",
      "  To change the order, set the position to a positive integer.\n",
      "- `config` _Any_ - the config to be passed to the reply function.\n",
      "  When an agent is reset, the config will be reset to the original value.\n",
      "- `reset_config` _Callable_ - the function to reset the config.\n",
      "  The function returns None. Signature: ```def reset_config(config: Any)```\n",
      "\n",
      "#### system\\_message\n",
      "\n",
      "```python\n",
      "@property\n",
      "def system_message()\n",
      "```\n",
      "\n",
      "Return the system message.\n",
      "\n",
      "#### update\\_system\\_message\n",
      "\n",
      "```python\n",
      "def update_system_message(system_message: str)\n",
      "```\n",
      "\n",
      "Update the system message.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "\n",
      "#### update\\_max\\_consecutive\\_auto\\_reply\n",
      "\n",
      "```python\n",
      "def update_max_consecutive_auto_reply(value: int,\n",
      "                                      sender: Optional[Agent] = None)\n",
      "```\n",
      "\n",
      "Update the maximum number of consecutive auto replies.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `value` _int_ - the maximum number of consecutive auto replies.\n",
      "- `sender` _Agent_ - when the sender is provided, only update the max_consecutive_auto_reply for that sender.\n",
      "\n",
      "#### max\\_consecutive\\_auto\\_reply\n",
      "\n",
      "```python\n",
      "def max_consecutive_auto_reply(sender: Optional[Agent] = None) -> int\n",
      "```\n",
      "\n",
      "The maximum number of consecutive auto replies.\n",
      "\n",
      "#### chat\\_messages\n",
      "\n",
      "```python\n",
      "@property\n",
      "def chat_messages() -> Dict[Agent, List[Dict]]\n",
      "```\n",
      "\n",
      "A dictionary of conversations from agent to list of messages.\n",
      "\n",
      "#### last\\_message\n",
      "\n",
      "```python\n",
      "def last_message(agent: Optional[Agent] = None) -> Dict\n",
      "```\n",
      "\n",
      "The last message exchanged with the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `agent` _Agent_ - The agent in the conversation.\n",
      "  If None and more than one agent's conversations are found, an error will be raised.\n",
      "  If None and only one conversation is found, the last message of the only conversation will be returned.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "  The last message exchanged with the agent.\n",
      "\n",
      "#### use\\_docker\n",
      "\n",
      "```python\n",
      "@property\n",
      "def use_docker() -> Union[bool, str, None]\n",
      "```\n",
      "\n",
      "Bool value of whether to use docker to execute the code,\n",
      "or str value of the docker image name to use, or None when code execution is disabled.\n",
      "\n",
      "#### send\n",
      "\n",
      "```python\n",
      "def send(message: Union[Dict, str],\n",
      "         recipient: Agent,\n",
      "         request_reply: Optional[bool] = None,\n",
      "         silent: Optional[bool] = False) -> bool\n",
      "```\n",
      "\n",
      "Send a message to another agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "- `message` _dict or str_ - the message to send.\n",
      "  If a dictionary is provided, it should contain the following keys:\n",
      "  - \"content\" (str): the content of the message.\n",
      "  - \"role\" (str): the role of the sender (\"system\", \"user\", \"assistant\", etc.).\n",
      "  - \"name\" (str): the name of the sender.\n",
      "  If a string is provided, it will be used as the content of the message,\n",
      "  and the role and name of the sender will be set to \"user\" and the agent's name, respectively.\n",
      "- `recipient` _Agent_ - the recipient agent.\n",
      "- `request_reply` _bool, optional_ - whether to request a reply from the recipient.\n",
      "  If None, the recipient's default value will be used.\n",
      "- `silent` _bool, optional_ - whether to suppress the output of the send operation. Default to False.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `bool` - True if the message is successfully sent, False otherwise.\n",
      "\n",
      "#### reply\n",
      "\n",
      "```python\n",
      "def reply(messages: List,\n",
      "          sender: Optional[Agent] = None,\n",
      "          silent: Optional[bool] = False)\n",
      "```\n",
      "\n",
      "Reply to the sender of the last message.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `messages` _list_ - the messages to send as the reply.\n",
      "- `sender` _Agent, optional_ - the sender of the reply.\n",
      "  If None, the last sender will be used.\n",
      "- `silent` _bool, optional_ - whether to suppress the output of the reply operation. Default to False.\n",
      "\n",
      "  Usually used inside a `register_reply`'s reply function.\n",
      "\n",
      "#### multi_send\n",
      "\n",
      "```python\n",
      "def multi_send(messages: List[Dict],\n",
      "               recipients: List[Agent],\n",
      "               request_reply: Optional[bool] = None,\n",
      "               silent: Optional[bool] = False) -> bool\n",
      "```\n",
      "\n",
      "Send the same message to multiple recipients.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `messages` _list_ - the messages to send.\n",
      "- `recipients` _list_ - the recipient agents.\n",
      "- `request_reply` _bool, optional_ - whether to request a reply from the recipients.\n",
      "  If None, the recipients' default value will be used.\n",
      "- `silent` _bool, optional_ - whether to suppress the output of the multi_send operation. Default to False.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `bool` - True if the messages are successfully sent, False otherwise.\n",
      "\n",
      "#### get\\_human\\_input\n",
      "\n",
      "```python\n",
      "def get_human_input(agent: Optional[Agent] = None) -> str\n",
      "```\n",
      "\n",
      "Get human input for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `agent` _Agent, optional_ - the agent to get human input from.\n",
      "  If None and more than one agent's conversations are found, an error will be raised.\n",
      "  If None and only one conversation is found, human input of the only conversation will be returned.\n",
      "  \n",
      "**Returns**:\n",
      "\n",
      "  The human input for the agent.\n",
      "\n",
      "#### generate\\_reply\n",
      "\n",
      "```python\n",
      "def generate_reply(messages: List[Dict],\n",
      "                   sender: Optional[Agent] = None,\n",
      "                   config: Optional[Any] = None) -> Union[str, Dict, None]\n",
      "```\n",
      "\n",
      "Generate a reply based on the messages.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `messages` _list_ - the received messages.\n",
      "- `sender` _Agent, optional_ - the sender of the reply.\n",
      "  If None, the last sender will be used.\n",
      "- `config` _Any, optional_ - the config to be passed to the reply function.\n",
      "\n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str, dict, None` - the reply message.\n",
      "\n",
      "#### execute\\_code\\_blocks\n",
      "\n",
      "```python\n",
      "def execute_code_blocks(messages: List[Dict],\n",
      "                        sender: Optional[Agent] = None) -> Union[str, None]\n",
      "```\n",
      "\n",
      "Execute code blocks in the messages.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `messages` _list_ - the received messages.\n",
      "- `sender` _Agent, optional_ - the sender of the code blocks.\n",
      "  If None, the last sender will be used.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str, None` - the result of executing the code blocks.\n",
      "\n",
      "#### execute\\_function\n",
      "\n",
      "```python\n",
      "def execute_function(function_name: str,\n",
      "                     params: Optional[Union[List, Dict]] = None,\n",
      "                     sender: Optional[Agent] = None) -> Union[str, None]\n",
      "```\n",
      "\n",
      "Execute a function in the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `function_name` _str_ - the name of the function to execute.\n",
      "- `params` _list or dict, optional_ - the parameters for the function.\n",
      "- `sender` _Agent, optional_ - the sender of the function call.\n",
      "  If None, the last sender will be used.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str, None` - the result of executing the function.\n",
      "\n",
      "#### execute\\_python\\_code\n",
      "\n",
      "```python\n",
      "def execute_python_code(code: str,\n",
      "                        sender: Optional[Agent] = None) -> Union[str, None]\n",
      "```\n",
      "\n",
      "Execute a single block of python code.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `code` _str_ - the python code to execute.\n",
      "- `sender` _Agent, optional_ - the sender of the code.\n",
      "  If None, the last sender will be used.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str, None` - the result of executing the python code.\n",
      "\n",
      "#### execute\\_llm\n",
      "\n",
      "```python\n",
      "def execute_llm(messages: List[Dict],\n",
      "                sender: Optional[Agent] = None) -> Union[str, None]\n",
      "```\n",
      "\n",
      "Get LLM based reply for given LLM messages.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `messages` _list_ - the received messages.\n",
      "- `sender` _Agent, optional_ - the sender of the llm messages.\n",
      "  If None, the last sender will be used.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str, None` - the LLM based reply.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3990279132.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    silent: Optional[bool] = False)\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "None\n",
      "expected ':' (3990279132.py, line 3)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "It appears that there is a syntax error in your code. The error message suggests that there is a missing colon (':') on line 3 of your code. Please review your code and make sure that all necessary colons are present. If you need further assistance, please provide the code that you are trying to execute.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Is there a function called tune_automl?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"example-2\"></a>\n",
    "### Example 2\n",
    "\n",
    "[back to top](#toc)\n",
    "\n",
    "Use RetrieveChat to answer a question that is not related to code generation.\n",
    "\n",
    "Problem: Who is the author of FLAML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAdding doc_id 0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 2 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 13 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 16 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 18 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 1 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 15 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 12 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 3 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 4 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 8 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id 10 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Who is the author of FLAML?\n",
      "\n",
      "Context is: {\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"items\": [\n",
      "        {\n",
      "          \"items\": [\n",
      "            \"reference/agentchat/contrib/math_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/qdrant_retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_assistant_agent\",\n",
      "            \"reference/agentchat/contrib/retrieve_user_proxy_agent\",\n",
      "            \"reference/agentchat/contrib/teachable_agent\",\n",
      "            \"reference/agentchat/contrib/text_analyzer_agent\"\n",
      "          ],\n",
      "          \"label\": \"agentchat.contrib\",\n",
      "          \"type\": \"category\"\n",
      "        },\n",
      "        \"reference/agentchat/agent\",\n",
      "        \"reference/agentchat/assistant_agent\",\n",
      "        \"reference/agentchat/conversable_agent\",\n",
      "        \"reference/agentchat/groupchat\",\n",
      "        \"reference/agentchat/user_proxy_agent\"\n",
      "      ],\n",
      "      \"label\": \"agentchat\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    {\n",
      "      \"items\": [\n",
      "        \"reference/oai/completion\",\n",
      "        \"reference/oai/openai_utils\"\n",
      "      ],\n",
      "      \"label\": \"oai\",\n",
      "      \"type\": \"category\"\n",
      "    },\n",
      "    \"reference/code_utils\",\n",
      "    \"reference/math_utils\",\n",
      "    \"reference/retrieve_utils\",\n",
      "    \"reference/token_count_utils\"\n",
      "  ],\n",
      "  \"label\": \"Reference\",\n",
      "  \"type\": \"category\"\n",
      "}\n",
      "---\n",
      "sidebar_label: math_utils\n",
      "title: math_utils\n",
      "---\n",
      "\n",
      "#### solve\\_problem\n",
      "\n",
      "```python\n",
      "def solve_problem(problem: str, **config) -> str\n",
      "```\n",
      "\n",
      "(Experimental) Solve the math problem.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - The problem statement.\n",
      "- `config` _Optional, dict_ - The configuration for the API call.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The solution to the problem.\n",
      "\n",
      "#### remove\\_boxed\n",
      "\n",
      "```python\n",
      "def remove_boxed(string: str) -> Optional[str]\n",
      "```\n",
      "\n",
      "Source: https://github.com/hendrycks/math\n",
      "Extract the text within a \\boxed{...} environment.\n",
      "\n",
      "**Example**:\n",
      "\n",
      "  \n",
      "  > remove_boxed(\"\\boxed{\\frac{2}{3}}\")\n",
      "  \n",
      "  \\frac{2}{3}\n",
      "\n",
      "#### last\\_boxed\\_only\\_string\n",
      "\n",
      "```python\n",
      "def last_boxed_only_string(string: str) -> Optional[str]\n",
      "```\n",
      "\n",
      "Source: https://github.com/hendrycks/math\n",
      "Extract the last \\boxed{...} or \\fbox{...} element from a string.\n",
      "\n",
      "#### is\\_equiv\n",
      "\n",
      "```python\n",
      "def is_equiv(str1: Optional[str], str2: Optional[str]) -> float\n",
      "```\n",
      "\n",
      "Returns (as a float) whether two strings containing math are equivalent up to differences of formatting in\n",
      "- units\n",
      "- fractions\n",
      "- square roots\n",
      "- superfluous LaTeX.\n",
      "Source: https://github.com/hendrycks/math\n",
      "\n",
      "#### is\\_equiv\\_chain\\_of\\_thought\n",
      "\n",
      "```python\n",
      "def is_equiv_chain_of_thought(str1: str, str2: str) -> float\n",
      "```\n",
      "\n",
      "Strips the solution first before calling `is_equiv`.\n",
      "\n",
      "#### eval\\_math\\_responses\n",
      "\n",
      "```python\n",
      "def eval_math_responses(responses, solution=None, **args)\n",
      "```\n",
      "\n",
      "Select a response for a math problem using voting, and check if the response is correct if the solution is provided.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `responses` _list_ - The list of responses.\n",
      "- `solution` _str_ - The canonical solution.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `dict` - The success metrics.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: qdrant_retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.qdrant_retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## QdrantRetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             retrieve_config: Optional[Dict] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, qdrant_client.QdrantClient(\":memory:\")): A QdrantClient instance. If not provided, an in-memory instance will be assigned. Not recommended for production.\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `BAAI/bge-small-en-v1.5` will be used. All available models\n",
      "  can be found at `https://qdrant.github.io/fastembed/examples/Supported_Models/`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take a string as input and return three integers (token_count, tokens_per_message, tokens_per_name).\n",
      "  Default is None, tiktoken will be used and may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "  - parallel (Optional, int): How many parallel workers to use for embedding. Defaults to the number of CPU cores.\n",
      "  - on_disk (Optional, bool): Whether to store the collection on disk. Default is False.\n",
      "  - quantization_config: Quantization configuration. If None, quantization will be disabled.\n",
      "  - hnsw_config: HNSW configuration. If None, default configuration will be used.\n",
      "  You can find more info about the hnsw configuration options at https://qdrant.tech/documentation/concepts/indexing/`vector`-index.\n",
      "  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_collection\n",
      "  - payload_indexing: Whether to create a payload index for the document field. Default is False.\n",
      "  You can find more info about the payload indexing options at https://qdrant.tech/documentation/concepts/indexing/`payload`-index\n",
      "  API Reference: https://qdrant.github.io/qdrant/redoc/index.html#tag/collections/operation/create_field_index\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### create\\_qdrant\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def create_qdrant_from_dir(\n",
      "        dir_path: str,\n",
      "        max_tokens: int = 4000,\n",
      "        client: QdrantClient = None,\n",
      "        collection_name: str = \"all-my-documents\",\n",
      "        chunk_mode: str = \"multi_lines\",\n",
      "        must_break_at_empty_line: bool = True,\n",
      "        embedding_model: str = \"BAAI/bge-small-en-v1.5\",\n",
      "        custom_text_split_function: Callable = None,\n",
      "        parallel: int = 0,\n",
      "        on_disk: bool = False,\n",
      "        quantization_config: Optional[models.QuantizationConfig] = None,\n",
      "        hnsw_config: Optional[models.HnswConfigDiff] = None,\n",
      "        payload_indexing: bool = False,\n",
      "        qdrant_client_options: Optional[Dict] = {})\n",
      "```\n",
      "\n",
      "Create a Qdrant collection from all the files in a given directory, the directory can also be a single file or a url to\n",
      "a single file.\n",
      "\n",
      "**Arguments**:\n",
      "---\n",
      "sidebar_label: retrieve_user_proxy_agent\n",
      "title: agentchat.contrib.retrieve_user_proxy_agent\n",
      "---\n",
      "\n",
      "## RetrieveUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"RetrieveChatAgent\",\n",
      "             human_input_mode: Optional[str] = \"ALWAYS\",\n",
      "             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n",
      "             retrieve_config: Optional[Dict] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary\n",
      "  and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `retrieve_config` _dict or None_ - config for the retrieve agent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
      "  - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
      "  prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
      "  - client (Optional, chromadb.Client): the chromadb client. If key not provided, a default client `chromadb.Client()`\n",
      "  will be used. If you want to use other vector db, extend this class and override the `retrieve_docs` function.\n",
      "  - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
      "  or the url to a single file. Default is None, which works only if the collection is already created.\n",
      "  - collection_name (Optional, str): the name of the collection.\n",
      "  If key not provided, a default name `autogen-docs` will be used.\n",
      "  - model (Optional, str): the model to use for the retrieve chat.\n",
      "  If key not provided, a default model `gpt-4` will be used.\n",
      "  - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.4` will be used.\n",
      "  - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
      "  If key not provided, a default size `max_tokens * 0.8` will be used.\n",
      "  - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
      "  \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
      "  - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
      "  If chunk_mode is \"one_line\", this parameter will be ignored.\n",
      "  - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
      "  If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\n",
      "  can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\n",
      "  fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\n",
      "  - embedding_function (Optional, Callable): the embedding function for creating the vector db. Default is None,\n",
      "  SentenceTransformer with the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or\n",
      "  other embedding functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
      "  - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
      "  If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
      "  - update_context (Optional, bool): if False, will not apply `Update Context` for interactive retrieval. Default is True.\n",
      "  - get_or_create (Optional, bool): if True, will create/recreate a collection for the retrieve chat.\n",
      "  This is the same as that used in chromadb. Default is False. Will be set to False if docs_path is None.\n",
      "  - custom_token_count_function(Optional, Callable): a custom function to count the number of tokens in a string.\n",
      "  The function should take (text:str, model:str) as input and return the token_count(int). the retrieve_config[\"model\"] will be passed in the function.\n",
      "  Default is autogen.token_count_utils.count_token that uses tiktoken, which may not be accurate for non-OpenAI models.\n",
      "  - custom_text_split_function(Optional, Callable): a custom function to split a string into a list of strings.\n",
      "  Default is None, will use the default function in `autogen.retrieve_utils.split_text_to_chunks`.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "  \n",
      "  Example of overriding retrieve_docs:\n",
      "  If you have set up a customized vector db, and it's not compatible with chromadb, you can easily plug in it with below code.\n",
      "```python\n",
      "class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n",
      "    def query_vector_db(\n",
      "        self,\n",
      "        query_texts: List[str],\n",
      "        n_results: int = 10,\n",
      "        search_string: str = \"\",\n",
      "        **kwargs,\n",
      "    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
      "        # define your own query function here\n",
      "        pass\n",
      "\n",
      "    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
      "        results = self.query_vector_db(\n",
      "            query_texts=[problem],\n",
      "            n_results=n_results,\n",
      "            search_string=search_string,\n",
      "            **kwargs,\n",
      "        )\n",
      "\n",
      "        self._results = results\n",
      "        print(\"doc_ids: \", results[\"ids\"])\n",
      "```\n",
      "\n",
      "#### retrieve\\_docs\n",
      "\n",
      "```python\n",
      "def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Retrieve docs based on the given problem and assign the results to the class property `_results`.\n",
      "In case you want to customize the retrieval process, such as using a different vector db whose APIs are not\n",
      "compatible with chromadb or filter results with metadata, you can override this function. Just keep the current\n",
      "parameters and add your own parameters with default values, and keep the results in below type.\n",
      "\n",
      "Type of the results: Dict[str, List[List[Any]]], should have keys \"ids\" and \"documents\", \"ids\" for the ids of\n",
      "the retrieved docs and \"documents\" for the contents of the retrieved docs. Any other keys are optional. Refer\n",
      "to `chromadb.api.types.QueryResult` as an example.\n",
      "ids: List[string]\n",
      "documents: List[List[string]]\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem: str,\n",
      "                          n_results: int = 20,\n",
      "                          search_string: str = \"\")\n",
      "```\n",
      "\n",
      "Generate an initial message with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `n_results` _int_ - the number of results to be retrieved.\n",
      "- `search_string` _str_ - only docs containing this string will be retrieved.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: text_analyzer_agent\n",
      "title: agentchat.contrib.text_analyzer_agent\n",
      "---\n",
      "\n",
      "## TextAnalyzerAgent Objects\n",
      "\n",
      "```python\n",
      "class TextAnalyzerAgent(ConversableAgent)\n",
      "```\n",
      "\n",
      "Text Analysis agent, a subclass of ConversableAgent designed to analyze text as instructed.\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name=\"analyzer\",\n",
      "             system_message: Optional[str] = system_message,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             llm_config: Optional[Union[Dict, bool]] = None,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent.\n",
      "- `system_message` _str_ - system message for the ChatCompletion inference.\n",
      "- `human_input_mode` _str_ - This agent should NEVER prompt the human for input.\n",
      "- `llm_config` _dict or False_ - llm inference configuration.\n",
      "  Please refer to [Completion.create](/docs/reference/oai/completion#create)\n",
      "  for available options.\n",
      "  To disable llm-based auto reply, set to False.\n",
      "- `teach_config` _dict or None_ - Additional parameters used by TeachableAgent.\n",
      "  To use default config, set to None. Otherwise, set to a dictionary with any of the following keys:\n",
      "  - verbosity (Optional, int): # 0 (default) for basic info, 1 to add memory operations, 2 for analyzer messages, 3 for memo lists.\n",
      "  - reset_db (Optional, bool): True to clear the DB before starting. Default False.\n",
      "  - path_to_db_dir (Optional, str): path to the directory where the DB is stored. Default \"./tmp/teachable_agent_db\"\n",
      "  - prepopulate (Optional, int): True (default) to prepopulate the DB with a set of input-output pairs.\n",
      "  - recall_threshold (Optional, float): The maximum distance for retrieved memos, where 0.0 is exact match. Default 1.5. Larger values allow more (but less relevant) memos to be recalled.\n",
      "  - max_num_retrievals (Optional, int): The maximum number of memos to retrieve from the DB. Default 10.\n",
      "- `**kwargs` _dict_ - other kwargs in [ConversableAgent](../conversable_agent#__init__).\n",
      "\n",
      "#### analyze\\_text\n",
      "\n",
      "```python\n",
      "def analyze_text(text_to_analyze, analysis_instructions)\n",
      "```\n",
      "\n",
      "Analyzes the given text as instructed, and returns the analysis.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: code_utils\n",
      "title: code_utils\n",
      "---\n",
      "\n",
      "#### infer\\_lang\n",
      "\n",
      "```python\n",
      "def infer_lang(code)\n",
      "```\n",
      "\n",
      "infer the language for the code.\n",
      "TODO: make it robust.\n",
      "\n",
      "#### extract\\_code\n",
      "\n",
      "```python\n",
      "def extract_code(\n",
      "        text: str,\n",
      "        pattern: str = CODE_BLOCK_PATTERN,\n",
      "        detect_single_line_code: bool = False) -> List[Tuple[str, str]]\n",
      "```\n",
      "\n",
      "Extract code from a text.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `text` _str_ - The text to extract code from.\n",
      "- `pattern` _str, optional_ - The regular expression pattern for finding the\n",
      "  code block. Defaults to CODE_BLOCK_PATTERN.\n",
      "- `detect_single_line_code` _bool, optional_ - Enable the new feature for\n",
      "  extracting single line code. Defaults to False.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `list` - A list of tuples, each containing the language and the code.\n",
      "  If there is no code block in the input text, the language would be \"unknown\".\n",
      "  If there is code block but the language is not specified, the language would be \"\".\n",
      "\n",
      "#### generate\\_code\n",
      "\n",
      "```python\n",
      "def generate_code(pattern: str = CODE_BLOCK_PATTERN,\n",
      "                  **config) -> Tuple[str, float]\n",
      "```\n",
      "\n",
      "Generate code.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `pattern` _Optional, str_ - The regular expression pattern for finding the code block.\n",
      "  The default pattern is for finding a code block in a markdown file.\n",
      "- `config` _Optional, dict_ - The configuration for the API call.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The generated code.\n",
      "- `float` - The cost of the generation.\n",
      "\n",
      "#### improve\\_function\n",
      "\n",
      "```python\n",
      "def improve_function(file_name, func_name, objective, **config)\n",
      "```\n",
      "\n",
      "(work in progress) Improve the function to achieve the objective.\n",
      "\n",
      "#### improve\\_code\n",
      "\n",
      "```python\n",
      "def improve_code(files, objective, suggest_only=True, **config)\n",
      "```\n",
      "\n",
      "Improve the code to achieve a given objective.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `files` _list_ - A list of file names containing the source code.\n",
      "- `objective` _str_ - The objective to achieve.\n",
      "- `suggest_only` _bool_ - Whether to return only the suggestions or the improved code.\n",
      "- `config` _Optional, dict_ - The configuration for the API call.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The improved code if suggest_only=False; a list of suggestions if suggest_only=True (default).\n",
      "- `float` - The cost of the generation.\n",
      "\n",
      "#### execute\\_code\n",
      "\n",
      "```python\n",
      "def execute_code(code: Optional[str] = None,\n",
      "                 timeout: Optional[int] = None,\n",
      "                 filename: Optional[str] = None,\n",
      "                 work_dir: Optional[str] = None,\n",
      "                 use_docker: Optional[Union[List[str], str, bool]] = None,\n",
      "                 lang: Optional[str] = \"python\") -> Tuple[int, str, str]\n",
      "```\n",
      "\n",
      "Execute code in a docker container.\n",
      "This function is not tested on MacOS.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `code` _Optional, str_ - The code to execute.\n",
      "  If None, the code from the file specified by filename will be executed.\n",
      "  Either code or filename must be provided.\n",
      "- `timeout` _Optional, int_ - The maximum execution time in seconds.\n",
      "  If None, a default timeout will be used. The default timeout is 600 seconds. On Windows, the timeout is not enforced when use_docker=False.\n",
      "- `filename` _Optional, str_ - The file name to save the code or where the code is stored when `code` is None.\n",
      "  If None, a file with a randomly generated name will be created.\n",
      "  The randomly generated file will be deleted after execution.\n",
      "  The file name must be a relative path. Relative paths are relative to the working directory.\n",
      "- `work_dir` _Optional, str_ - The working directory for the code execution.\n",
      "  If None, a default working directory will be used.\n",
      "  The default working directory is the \"extensions\" directory under\n",
      "  \"path_to_autogen\".\n",
      "- `use_docker` _Optional, list, str or bool_ - The docker image to use for code execution.\n",
      "  If a list or a str of image name(s) is provided, the code will be executed in a docker container\n",
      "  with the first image successfully pulled.\n",
      "  If None, False or empty, the code will be executed in the current environment.\n",
      "  Default is None, which will be converted into an empty list when docker package is available.\n",
      "  Expected behaviour:\n",
      "  - If `use_docker` is explicitly set to True and the docker package is available, the code will run in a Docker container.\n",
      "  - If `use_docker` is explicitly set to True but the Docker package is missing, an error will be raised.\n",
      "  - If `use_docker` is not set (i.e., left default to None) and the Docker package is not available, a warning will be displayed, but the code will run natively.\n",
      "  If the code is executed in the current environment,\n",
      "  the code must be trusted.\n",
      "- `lang` _Optional, str_ - The language of the code. Default is \"python\".\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `int` - 0 if the code executes successfully.\n",
      "- `str` - The error message if the code fails to execute; the stdout otherwise.\n",
      "- `image` - The docker image name after container run when docker is used.\n",
      "\n",
      "#### generate\\_assertions\n",
      "\n",
      "```python\n",
      "def generate_assertions(definition: str, **config) -> Tuple[str, float]\n",
      "```\n",
      "\n",
      "Generate assertions for a function.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `definition` _str_ - The function definition, including the signature and docstr.\n",
      "- `config` _Optional, dict_ - The configuration for the API call.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The generated assertions.\n",
      "- `float` - The cost of the generation.\n",
      "\n",
      "#### eval\\_function\\_completions\n",
      "\n",
      "```python\n",
      "def eval_function_completions(responses: List[str],\n",
      "                              definition: str,\n",
      "                              test: Optional[str] = None,\n",
      "                              entry_point: Optional[str] = None,\n",
      "                              assertions: Optional[Union[str, Callable[\n",
      "                                  [str], Tuple[str, float]]]] = None,\n",
      "                              timeout: Optional[float] = 3,\n",
      "                              use_docker: Optional[bool] = True) -> Dict\n",
      "```\n",
      "\n",
      "Select a response from a list of responses for the function completion task (using generated assertions), and/or evaluate if the task is successful using a gold test.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `responses` _list_ - The list of responses.\n",
      "- `definition` _str_ - The input definition.\n",
      "- `test` _Optional, str_ - The test code.\n",
      "- `entry_point` _Optional, str_ - The name of the function.\n",
      "- `assertions` _Optional, str or Callable_ - The assertion code which serves as a filter of the responses, or an assertion generator.\n",
      "  When provided, only the responses that pass the assertions will be considered for the actual test (if provided).\n",
      "- `timeout` _Optional, float_ - The timeout for executing the code.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `dict` - The success metrics.\n",
      "\n",
      "## PassAssertionFilter Objects\n",
      "\n",
      "```python\n",
      "class PassAssertionFilter()\n",
      "```\n",
      "\n",
      "#### pass\\_assertions\n",
      "\n",
      "```python\n",
      "def pass_assertions(context, response, **_)\n",
      "```\n",
      "\n",
      "Check if the response passes the assertions.\n",
      "\n",
      "#### implement\n",
      "\n",
      "```python\n",
      "def implement(\n",
      "    definition: str,\n",
      "    configs: Optional[List[Dict]] = None,\n",
      "    assertions: Optional[Union[str,\n",
      "                               Callable[[str],\n",
      "                                        Tuple[str,\n",
      "                                              float]]]] = generate_assertions\n",
      ") -> Tuple[str, float]\n",
      "```\n",
      "\n",
      "Implement a function from a definition.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `definition` _str_ - The function definition, including the signature and docstr.\n",
      "- `configs` _list_ - The list of configurations for completion.\n",
      "- `assertions` _Optional, str or Callable_ - The assertion code which serves as a filter of the responses, or an assertion generator.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - The implementation.\n",
      "- `float` - The cost of the implementation.\n",
      "- `int` - The index of the configuration which generates the implementation.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: retrieve_assistant_agent\n",
      "title: agentchat.contrib.retrieve_assistant_agent\n",
      "---\n",
      "\n",
      "## RetrieveAssistantAgent Objects\n",
      "\n",
      "```python\n",
      "class RetrieveAssistantAgent(AssistantAgent)\n",
      "```\n",
      "\n",
      "(Experimental) Retrieve Assistant agent, designed to solve a task with LLM.\n",
      "\n",
      "RetrieveAssistantAgent is a subclass of AssistantAgent configured with a default system message.\n",
      "The default system message is designed to solve a task with LLM,\n",
      "including suggesting python code blocks and debugging.\n",
      "`human_input_mode` is default to \"NEVER\"\n",
      "and `code_execution_config` is default to False.\n",
      "This agent doesn't execute code by default, and expects the user to execute the code.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: math_user_proxy_agent\n",
      "title: agentchat.contrib.math_user_proxy_agent\n",
      "---\n",
      "\n",
      "## MathUserProxyAgent Objects\n",
      "\n",
      "```python\n",
      "class MathUserProxyAgent(UserProxyAgent)\n",
      "```\n",
      "\n",
      "(Experimental) A MathChat agent that can handle math problems.\n",
      "\n",
      "#### MAX\\_CONSECUTIVE\\_AUTO\\_REPLY\n",
      "\n",
      "maximum number of consecutive auto replies (subject to future change)\n",
      "\n",
      "#### \\_\\_init\\_\\_\n",
      "\n",
      "```python\n",
      "def __init__(name: Optional[str] = \"MathChatAgent\",\n",
      "             is_termination_msg: Optional[Callable[\n",
      "                 [Dict], bool]] = _is_termination_msg_mathchat,\n",
      "             human_input_mode: Optional[str] = \"NEVER\",\n",
      "             default_auto_reply: Optional[Union[str, Dict,\n",
      "                                                None]] = DEFAULT_REPLY,\n",
      "             max_invalid_q_per_step=3,\n",
      "             **kwargs)\n",
      "```\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `name` _str_ - name of the agent\n",
      "- `is_termination_msg` _function_ - a function that takes a message in the form of a dictionary and returns a boolean value indicating if this received message is a termination message.\n",
      "  The dict can contain the following keys: \"content\", \"role\", \"name\", \"function_call\".\n",
      "- `human_input_mode` _str_ - whether to ask for human inputs every time a message is received.\n",
      "  Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
      "  (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
      "  Under this mode, the conversation stops when the human input is \"exit\",\n",
      "  or when is_termination_msg is True and there is no human input.\n",
      "  (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
      "  the number of auto reply reaches the max_consecutive_auto_reply.\n",
      "  (3) (Default) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
      "  when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
      "- `default_auto_reply` _str or dict or None_ - the default auto reply message when no code execution or llm based reply is generated.\n",
      "- `max_invalid_q_per_step` _int_ - (ADDED) the maximum number of invalid queries per step.\n",
      "- `**kwargs` _dict_ - other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
      "\n",
      "#### generate\\_init\\_message\n",
      "\n",
      "```python\n",
      "def generate_init_message(problem,\n",
      "                          prompt_type=\"default\",\n",
      "                          customized_prompt=None)\n",
      "```\n",
      "\n",
      "Generate a prompt for the assistant agent with the given problem and prompt.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `problem` _str_ - the problem to be solved.\n",
      "- `prompt_type` _str_ - the type of the prompt. Possible values are \"default\", \"python\", \"wolfram\".\n",
      "  (1) \"default\": the prompt that allows the agent to choose between 3 ways to solve a problem:\n",
      "  1. write a python program to solve it directly.\n",
      "  2. solve it directly without python.\n",
      "  3. solve it step by step with python.\n",
      "  (2) \"python\":\n",
      "  a simplified prompt from the third way of the \"default\" prompt, that asks the assistant\n",
      "  to solve the problem step by step with python.\n",
      "  (3) \"two_tools\":\n",
      "  a simplified prompt similar to the \"python\" prompt, but allows the model to choose between\n",
      "  Python and Wolfram Alpha to solve the problem.\n",
      "- `customized_prompt` _str_ - a customized prompt to be used. If it is not None, the prompt_type will be ignored.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `str` - the generated prompt ready to be sent to the assistant agent.\n",
      "\n",
      "#### execute\\_one\\_python\\_code\n",
      "\n",
      "```python\n",
      "def execute_one_python_code(pycode)\n",
      "```\n",
      "\n",
      "Execute python code blocks.\n",
      "\n",
      "Previous python code will be saved and executed together with the new code.\n",
      "the \"print\" function will also be added to the last line of the code if needed\n",
      "\n",
      "#### execute\\_one\\_wolfram\\_query\n",
      "\n",
      "```python\n",
      "def execute_one_wolfram_query(query: str)\n",
      "```\n",
      "\n",
      "Run one wolfram query and return the output.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query` - string of the query.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `output` - string with the output of the query.\n",
      "- `is_success` - boolean indicating whether the query was successful.\n",
      "\n",
      "#### get\\_from\\_dict\\_or\\_env\n",
      "\n",
      "```python\n",
      "def get_from_dict_or_env(data: Dict[str, Any],\n",
      "                         key: str,\n",
      "                         env_key: str,\n",
      "                         default: Optional[str] = None) -> str\n",
      "```\n",
      "\n",
      "Get a value from a dictionary or an environment variable.\n",
      "\n",
      "## WolframAlphaAPIWrapper Objects\n",
      "\n",
      "```python\n",
      "class WolframAlphaAPIWrapper(BaseModel)\n",
      "```\n",
      "\n",
      "Wrapper for Wolfram Alpha.\n",
      "\n",
      "Docs for using:\n",
      "\n",
      "1. Go to wolfram alpha and sign up for a developer account\n",
      "2. Create an app and get your APP ID\n",
      "3. Save your APP ID into WOLFRAM_ALPHA_APPID env variable\n",
      "4. pip install wolframalpha\n",
      "\n",
      "#### wolfram\\_client\n",
      "\n",
      ":meta private:\n",
      "\n",
      "## Config Objects\n",
      "\n",
      "```python\n",
      "class Config()\n",
      "```\n",
      "\n",
      "Configuration for this pydantic object.\n",
      "\n",
      "#### validate\\_environment\n",
      "\n",
      "```python\n",
      "@root_validator(skip_on_failure=True)\n",
      "def validate_environment(cls, values: Dict) -> Dict\n",
      "```\n",
      "\n",
      "Validate that api key and python package exists in environment.\n",
      "\n",
      "#### run\n",
      "\n",
      "```python\n",
      "def run(query: str) -> str\n",
      "```\n",
      "\n",
      "Run query through WolframAlpha and parse result.\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: retrieve_utils\n",
      "title: retrieve_utils\n",
      "---\n",
      "\n",
      "#### split\\_text\\_to\\_chunks\n",
      "\n",
      "```python\n",
      "def split_text_to_chunks(text: str,\n",
      "                         max_tokens: int = 4000,\n",
      "                         chunk_mode: str = \"multi_lines\",\n",
      "                         must_break_at_empty_line: bool = True,\n",
      "                         overlap: int = 10)\n",
      "```\n",
      "\n",
      "Split a long text into chunks of max_tokens.\n",
      "\n",
      "#### extract\\_text\\_from\\_pdf\n",
      "\n",
      "```python\n",
      "def extract_text_from_pdf(file: str) -> str\n",
      "```\n",
      "\n",
      "Extract text from PDF files\n",
      "\n",
      "#### split\\_files\\_to\\_chunks\n",
      "\n",
      "```python\n",
      "def split_files_to_chunks(files: list,\n",
      "                          max_tokens: int = 4000,\n",
      "                          chunk_mode: str = \"multi_lines\",\n",
      "                          must_break_at_empty_line: bool = True,\n",
      "                          custom_text_split_function: Callable = None)\n",
      "```\n",
      "\n",
      "Split a list of files into chunks of max_tokens.\n",
      "\n",
      "#### get\\_files\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def get_files_from_dir(dir_path: Union[str, List[str]],\n",
      "                       types: list = TEXT_FORMATS,\n",
      "                       recursive: bool = True)\n",
      "```\n",
      "\n",
      "Return a list of all the files in a given directory.\n",
      "\n",
      "#### get\\_file\\_from\\_url\n",
      "\n",
      "```python\n",
      "def get_file_from_url(url: str, save_path: str = None)\n",
      "```\n",
      "\n",
      "Download a file from a URL.\n",
      "\n",
      "#### is\\_url\n",
      "\n",
      "```python\n",
      "def is_url(string: str)\n",
      "```\n",
      "\n",
      "Return True if the string is a valid URL.\n",
      "\n",
      "#### create\\_vector\\_db\\_from\\_dir\n",
      "\n",
      "```python\n",
      "def create_vector_db_from_dir(\n",
      "        dir_path: str,\n",
      "        max_tokens: int = 4000,\n",
      "        client: API = None,\n",
      "        db_path: str = \"/tmp/chromadb.db\",\n",
      "        collection_name: str = \"all-my-documents\",\n",
      "        get_or_create: bool = False,\n",
      "        chunk_mode: str = \"multi_lines\",\n",
      "        must_break_at_empty_line: bool = True,\n",
      "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
      "        embedding_function: Callable = None,\n",
      "        custom_text_split_function: Callable = None) -> API\n",
      "```\n",
      "\n",
      "Create a vector db from all the files in a given directory, the directory can also be a single file or a url to\n",
      "a single file. We support chromadb compatible APIs to create the vector db, this function is not required if\n",
      "you prepared your own vector db.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `dir_path` _str_ - the path to the directory, file or url.\n",
      "- `max_tokens` _Optional, int_ - the maximum number of tokens per chunk. Default is 4000.\n",
      "- `client` _Optional, API_ - the chromadb client. Default is None.\n",
      "- `db_path` _Optional, str_ - the path to the chromadb. Default is \"/tmp/chromadb.db\".\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `get_or_create` _Optional, bool_ - Whether to get or create the collection. Default is False. If True, the collection\n",
      "  will be recreated if it already exists.\n",
      "- `chunk_mode` _Optional, str_ - the chunk mode. Default is \"multi_lines\".\n",
      "- `must_break_at_empty_line` _Optional, bool_ - Whether to break at empty line. Default is True.\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if\n",
      "  embedding_function is not None.\n",
      "- `embedding_function` _Optional, Callable_ - the embedding function to use. Default is None, SentenceTransformer with\n",
      "  the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n",
      "  functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `API` - the chromadb client.\n",
      "\n",
      "#### query\\_vector\\_db\n",
      "\n",
      "```python\n",
      "def query_vector_db(query_texts: List[str],\n",
      "                    n_results: int = 10,\n",
      "                    client: API = None,\n",
      "                    db_path: str = \"/tmp/chromadb.db\",\n",
      "                    collection_name: str = \"all-my-documents\",\n",
      "                    search_string: str = \"\",\n",
      "                    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
      "                    embedding_function: Callable = None) -> QueryResult\n",
      "```\n",
      "\n",
      "Query a vector db. We support chromadb compatible APIs, it's not required if you prepared your own vector db\n",
      "and query function.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `query_texts` _List[str]_ - the query texts.\n",
      "- `n_results` _Optional, int_ - the number of results to return. Default is 10.\n",
      "- `client` _Optional, API_ - the chromadb compatible client. Default is None, a chromadb client will be used.\n",
      "- `db_path` _Optional, str_ - the path to the vector db. Default is \"/tmp/chromadb.db\".\n",
      "- `collection_name` _Optional, str_ - the name of the collection. Default is \"all-my-documents\".\n",
      "- `search_string` _Optional, str_ - the search string. Default is \"\".\n",
      "- `embedding_model` _Optional, str_ - the embedding model to use. Default is \"all-MiniLM-L6-v2\". Will be ignored if\n",
      "  embedding_function is not None.\n",
      "- `embedding_function` _Optional, Callable_ - the embedding function to use. Default is None, SentenceTransformer with\n",
      "  the given `embedding_model` will be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\n",
      "  functions, you can pass it here, follow the examples in `https://docs.trychroma.com/embeddings`.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `QueryResult` - the query result. The format is:\n",
      "  class QueryResult(TypedDict):\n",
      "- `ids` - List[IDs]\n",
      "- `embeddings` - Optional[List[List[Embedding]]]\n",
      "- `documents` - Optional[List[List[Document]]]\n",
      "- `metadatas` - Optional[List[List[Metadata]]]\n",
      "- `distances` - Optional[List[List[float]]]\n",
      "\n",
      "\n",
      "---\n",
      "sidebar_label: token_count_utils\n",
      "title: token_count_utils\n",
      "---\n",
      "\n",
      "#### token\\_left\n",
      "\n",
      "```python\n",
      "def token_left(input: Union[str, List, Dict],\n",
      "               model=\"gpt-3.5-turbo-0613\") -> int\n",
      "```\n",
      "\n",
      "Count number of tokens left for an OpenAI model.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `input` - (str, list, dict): Input to the model.\n",
      "- `model` - (str): Model name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `int` - Number of tokens left that the model can use for completion.\n",
      "\n",
      "#### count\\_token\n",
      "\n",
      "```python\n",
      "def count_token(input: Union[str, List, Dict],\n",
      "                model: str = \"gpt-3.5-turbo-0613\") -> int\n",
      "```\n",
      "\n",
      "Count number of tokens used by an OpenAI model.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `input` - (str, list, dict): Input to the model.\n",
      "- `model` - (str): Model name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `int` - Number of tokens from the input.\n",
      "\n",
      "#### num\\_tokens\\_from\\_functions\n",
      "\n",
      "```python\n",
      "def num_tokens_from_functions(functions, model=\"gpt-3.5-turbo-0613\") -> int\n",
      "```\n",
      "\n",
      "Return the number of tokens used by a list of functions.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `functions` - (list): List of function descriptions that will be passed in model.\n",
      "- `model` - (str): Model name.\n",
      "  \n",
      "\n",
      "**Returns**:\n",
      "\n",
      "- `int` - Number of tokens from the function descriptions.\n",
      "\n",
      "\n",
      "\n",
      "- `message` _dict or str_ - message to be sent.\n",
      "  The message could contain the following fields:\n",
      "  - content (str): Required, the content of the message. (Can be None)\n",
      "  - function_call (str): the name of the function to be called.\n",
      "  - name (str): the name of the function to be called.\n",
      "  - role (str): the role of the message, any role that is not \"function\"\n",
      "  will be modified to \"assistant\".\n",
      "  - context (dict): the context of the message, which will be passed to\n",
      "  [Completion.create](../oai/Completion#create).\n",
      "  For example, one agent can send a message A as:\n",
      "```python\n",
      "{\n",
      "    \"content\": lambda context: context[\"use_tool_msg\"],\n",
      "    \"context\": {\n",
      "        \"use_tool_msg\": \"Use tool X if they are relevant.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "  Next time, one agent can send a message B with a different \"use_tool_msg\".\n",
      "  Then the content of message A will be refreshed to the new \"use_tool_msg\".\n",
      "  So effectively, this provides a way for an agent to send a \"link\" and modify\n",
      "  the content of the \"link\" later.\n",
      "- `recipient` _Agent_ - the recipient of the message.\n",
      "- `request_reply` _bool or None_ - whether to request a reply from the recipient.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the message sent.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `ValueError` - if the message can't be converted into a valid ChatCompletion message.\n",
      "\n",
      "#### a\\_send\n",
      "\n",
      "```python\n",
      "async def a_send(message: Union[Dict, str],\n",
      "                 recipient: Agent,\n",
      "                 request_reply: Optional[bool] = None,\n",
      "                 silent: Optional[bool] = False) -> bool\n",
      "```\n",
      "\n",
      "(async) Send a message to another agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `message` _dict or str_ - message to be sent.\n",
      "  The message could contain the following fields:\n",
      "  - content (str): Required, the content of the message. (Can be None)\n",
      "  - function_call (str): the name of the function to be called.\n",
      "  - name (str): the name of the function to be called.\n",
      "  - role (str): the role of the message, any role that is not \"function\"\n",
      "  will be modified to \"assistant\".\n",
      "  - context (dict): the context of the message, which will be passed to\n",
      "  [Completion.create](../oai/Completion#create).\n",
      "  For example, one agent can send a message A as:\n",
      "```python\n",
      "{\n",
      "    \"content\": lambda context: context[\"use_tool_msg\"],\n",
      "    \"context\": {\n",
      "        \"use_tool_msg\": \"Use tool X if they are relevant.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "  Next time, one agent can send a message B with a different \"use_tool_msg\".\n",
      "  Then the content of message A will be refreshed to the new \"use_tool_msg\".\n",
      "  So effectively, this provides a way for an agent to send a \"link\" and modify\n",
      "  the content of the \"link\" later.\n",
      "- `recipient` _Agent_ - the recipient of the message.\n",
      "- `request_reply` _bool or None_ - whether to request a reply from the recipient.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the message sent.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `ValueError` - if the message can't be converted into a valid ChatCompletion message.\n",
      "\n",
      "#### receive\n",
      "\n",
      "```python\n",
      "def receive(message: Union[Dict, str],\n",
      "            sender: Agent,\n",
      "            request_reply: Optional[bool] = None,\n",
      "            silent: Optional[bool] = False)\n",
      "```\n",
      "\n",
      "Receive a message from another agent.\n",
      "\n",
      "Once a message is received, this function sends a reply to the sender or stop.\n",
      "The reply can be generated automatically or entered manually by a human.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `message` _dict or str_ - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).\n",
      "  1. \"content\": content of the message, can be None.\n",
      "  2. \"function_call\": a dictionary containing the function name and arguments.\n",
      "  3. \"role\": role of the message, can be \"assistant\", \"user\", \"function\".\n",
      "  This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".\n",
      "  4. \"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.\n",
      "  5. \"context\" (dict): the context of the message, which will be passed to\n",
      "  [Completion.create](../oai/Completion#create).\n",
      "- `sender` - sender of an Agent instance.\n",
      "- `request_reply` _bool or None_ - whether a reply is requested from the sender.\n",
      "  If None, the value is determined by `self.reply_at_receive[sender]`.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the message received.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `ValueError` - if the message can't be converted into a valid ChatCompletion message.\n",
      "\n",
      "#### a\\_receive\n",
      "\n",
      "```python\n",
      "async def a_receive(message: Union[Dict, str],\n",
      "                    sender: Agent,\n",
      "                    request_reply: Optional[bool] = None,\n",
      "                    silent: Optional[bool] = False)\n",
      "```\n",
      "\n",
      "(async) Receive a message from another agent.\n",
      "\n",
      "Once a message is received, this function sends a reply to the sender or stop.\n",
      "The reply can be generated automatically or entered manually by a human.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `message` _dict or str_ - message from the sender. If the type is dict, it may contain the following reserved fields (either content or function_call need to be provided).\n",
      "  1. \"content\": content of the message, can be None.\n",
      "  2. \"function_call\": a dictionary containing the function name and arguments.\n",
      "  3. \"role\": role of the message, can be \"assistant\", \"user\", \"function\".\n",
      "  This field is only needed to distinguish between \"function\" or \"assistant\"/\"user\".\n",
      "  4. \"name\": In most cases, this field is not needed. When the role is \"function\", this field is needed to indicate the function name.\n",
      "  5. \"context\" (dict): the context of the message, which will be passed to\n",
      "  [Completion.create](../oai/Completion#create).\n",
      "- `sender` - sender of an Agent instance.\n",
      "- `request_reply` _bool or None_ - whether a reply is requested from the sender.\n",
      "  If None, the value is determined by `self.reply_at_receive[sender]`.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the message received.\n",
      "  \n",
      "\n",
      "**Raises**:\n",
      "\n",
      "- `ValueError` - if the message can't be converted into a valid ChatCompletion message.\n",
      "\n",
      "#### initiate\\_chat\n",
      "\n",
      "```python\n",
      "def initiate_chat(recipient: \"ConversableAgent\",\n",
      "                  clear_history: Optional[bool] = True,\n",
      "                  silent: Optional[bool] = False,\n",
      "                  **context)\n",
      "```\n",
      "\n",
      "Initiate a chat with the recipient agent.\n",
      "\n",
      "Reset the consecutive auto reply counter.\n",
      "If `clear_history` is True, the chat history with the recipient agent will be cleared.\n",
      "`generate_init_message` is called to generate the initial message for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `recipient` - the recipient agent.\n",
      "- `clear_history` _bool_ - whether to clear the chat history with the agent.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the messages for this conversation.\n",
      "- `**context` - any context information.\n",
      "  \"message\" needs to be provided if the `generate_init_message` method is not overridden.\n",
      "\n",
      "#### a\\_initiate\\_chat\n",
      "\n",
      "```python\n",
      "async def a_initiate_chat(recipient: \"ConversableAgent\",\n",
      "                          clear_history: Optional[bool] = True,\n",
      "                          silent: Optional[bool] = False,\n",
      "                          **context)\n",
      "```\n",
      "\n",
      "(async) Initiate a chat with the recipient agent.\n",
      "\n",
      "Reset the consecutive auto reply counter.\n",
      "If `clear_history` is True, the chat history with the recipient agent will be cleared.\n",
      "`generate_init_message` is called to generate the initial message for the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `recipient` - the recipient agent.\n",
      "- `clear_history` _bool_ - whether to clear the chat history with the agent.\n",
      "- `silent` _bool or None_ - (Experimental) whether to print the messages for this conversation.\n",
      "- `**context` - any context information.\n",
      "  \"message\" needs to be provided if the `generate_init_message` method is not overridden.\n",
      "\n",
      "#### reset\n",
      "\n",
      "```python\n",
      "def reset()\n",
      "```\n",
      "\n",
      "Reset the agent.\n",
      "\n",
      "#### stop\\_reply\\_at\\_receive\n",
      "\n",
      "```python\n",
      "def stop_reply_at_receive(sender: Optional[Agent] = None)\n",
      "```\n",
      "\n",
      "Reset the reply_at_receive of the sender.\n",
      "\n",
      "#### reset\\_consecutive\\_auto\\_reply\\_counter\n",
      "\n",
      "```python\n",
      "def reset_consecutive_auto_reply_counter(sender: Optional[Agent] = None)\n",
      "```\n",
      "\n",
      "Reset the consecutive_auto_reply_counter of the sender.\n",
      "\n",
      "#### clear\\_history\n",
      "\n",
      "```python\n",
      "def clear_history(agent: Optional[Agent] = None)\n",
      "```\n",
      "\n",
      "Clear the chat history of the agent.\n",
      "\n",
      "**Arguments**:\n",
      "\n",
      "- `agent` - the agent with whom the chat history to clear. If None, clear the chat history with all agents.\n",
      "\n",
      "#### generate\\_oai\\_reply\n",
      "\n",
      "```python\n",
      "def generate_oai_reply(\n",
      "        messages: Optional[List[Dict]] = None,\n",
      "        sender: Optional[Agent] = None,\n",
      "        config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, None]]\n",
      "```\n",
      "---\n",
      "sidebar_label: groupchat\n",
      "title: agentchat.groupchat\n",
      "---\n",
      "\n",
      "## GroupChat Objects\n",
      "\n",
      "```python\n",
      "@dataclass\n",
      "class GroupChat()\n",
      "```\n",
      "\n",
      "A group chat class that contains the following data fields:\n",
      "- agents: a list of participating agents.\n",
      "- messages: a list of messages in the group chat.\n",
      "- max_round: the maximum number of rounds.\n",
      "- admin_name: the name of the admin agent if there is one. Default is \"Admin\".\n",
      "    KeyBoardInterrupt will make the admin agent take over.\n",
      "- func_call_filter: whether to enforce function call filter. Default is True.\n",
      "    When set to True and when a message is a function call suggestion,\n",
      "    the next speaker will be chosen from an agent which contains the corresponding function name\n",
      "    in its `function_map`.\n",
      "\n",
      "#### agent\\_names\n",
      "\n",
      "```python\n",
      "@property\n",
      "def agent_names() -> List[str]\n",
      "```\n",
      "\n",
      "Return the names of the agents in the group chat.\n",
      "\n",
      "#### reset\n",
      "\n",
      "```python\n",
      "def reset()\n",
      "```\n",
      "\n",
      "Reset the group chat.\n",
      "\n",
      "#### agent\\_by\\_name\n",
      "\n",
      "```python\n",
      "def agent_by_name(name: str) -> Agent\n",
      "```\n",
      "\n",
      "Find the next speaker based on the message.\n",
      "\n",
      "#### next\\_agent\n",
      "\n",
      "```python\n",
      "def next_agent(agent: Agent, agents: List[Agent]) -> Agent\n",
      "```\n",
      "\n",
      "Return the next agent in the list.\n",
      "\n",
      "#### select\\_speaker\\_msg\n",
      "\n",
      "```python\n",
      "def select_speaker_msg(agents: List[Agent])\n",
      "```\n",
      "\n",
      "Return the message for selecting the next speaker.\n",
      "\n",
      "#### select\\_speaker\n",
      "\n",
      "```python\n",
      "def select_speaker(last_speaker: Agent, selector: ConversableAgent)\n",
      "```\n",
      "\n",
      "Select the next speaker.\n",
      "\n",
      "## GroupChatManager Objects\n",
      "\n",
      "```python\n",
      "class GroupChatManager(ConversableAgent)\n",
      "```\n",
      "\n",
      "(In preview) A chat manager agent that can manage a group chat of multiple agents.\n",
      "\n",
      "#### run\\_chat\n",
      "\n",
      "```python\n",
      "def run_chat(messages: Optional[List[Dict]] = None,\n",
      "             sender: Optional[Agent] = None,\n",
      "             config: Optional[GroupChat] = None) -> Union[str, Dict, None]\n",
      "```\n",
      "\n",
      "Run a group chat.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "The author of FLAML is Microsoft.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "qa_problem = \"Who is the author of FLAML?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=qa_problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
