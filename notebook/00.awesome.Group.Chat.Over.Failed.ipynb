{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        # \"model\": [\"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "        \"model\": [\"gpt-4\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": 42,\n",
    "    \"config_list\": config_list,\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 8192 * 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlBaseAddress = \"http://192.168.0.103:5000/SamplesData\"\n",
    "\n",
    "ordersDataUrl=f\"{urlBaseAddress}/Orders.csv\"\n",
    "\n",
    "customersDataUrl=f\"{urlBaseAddress}/Customers.csv\"\n",
    "\n",
    "customersGroupsDataUrl=f\"{urlBaseAddress}/CustomersGroups.csv\"\n",
    "\n",
    "productsDataUrl=f\"{urlBaseAddress}/Products.csv\"\n",
    "\n",
    "activitiesDataUrl=f\"{urlBaseAddress}/Activities.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_termination_msg_func(x):\n",
    "    r = x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\")\n",
    "    print(\"===============\", x)\n",
    "    # r = True\n",
    "    return r\n",
    "\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    # human_input_mode=\"TERMINATE\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    # is_termination_msg=lambda x: (x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\")),\n",
    "    is_termination_msg=is_termination_msg_func,\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config={\"work_dir\": \"00_awesome_poc\"},\n",
    "    system_message=\"\"\"\n",
    "    Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "    Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "planner = autogen.AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message='''\n",
    "    Planner. \n",
    "    Suggest a plan. Revise the plan based on feedback from admin and critic, until admin approval.\n",
    "    The plan may involve an engineer who can write code and others don't write code.\n",
    "    Explain the plan first. Be clear which step is performed by an engineer.\n",
    "    ''',\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "executor = autogen.UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\"last_n_messages\": 3, \"work_dir\": \"paper\"},\n",
    ")\n",
    "\n",
    "engineer = autogen.AssistantAgent(\n",
    "    name=\"Engineer\",\n",
    "    llm_config=llm_config,\n",
    "    system_message='''Engineer. You follow an approved plan. You write python/shell code to solve tasks. \n",
    "    Wrap the code in a code block that specifies the script type. \n",
    "    The user can't modify your code. So do not suggest incomplete code which requires others to modify. \n",
    "    Don't use a code block if it's not intended to be executed by the executor.\n",
    "    Don't include multiple code blocks in one response. Do not ask others to copy and paste the result. \n",
    "    Check the execution result returned by the executor.\n",
    "    If the result indicates there is an error, fix the error and output the code again. \n",
    "    Suggest the full code instead of partial code or code changes. \n",
    "    If the error can't be fixed or if the task is not solved even after the code is executed successfully, \n",
    "    analyze the problem, revisit your assumption, \n",
    "    collect additional info you need, and think of a different approach to try.\n",
    "''',\n",
    ")\n",
    "\n",
    "ordersAssistant = autogen.AssistantAgent(\n",
    "    name=\"OrdersAssistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"\n",
    "    OrdersAssistant. \n",
    "    你负责告知 \"Engineer\": \"Orders\" 订单原始数据，从如下地址下载：{ordersDataUrl}，\n",
    "    以及数据中的字段包括：OrderId,OrderNo,CustomerNo,Product,Price。\n",
    "    你不用编写代码。\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "productsAssistant = autogen.AssistantAgent(\n",
    "    name=\"ProductsAssistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message=f\"\"\"\n",
    "    ProductsAssistant. \n",
    "    你负责告知 \"Engineer\": \"Orders\" 订单原始数据，从如下地址下载：{productsDataUrl}，\n",
    "    以及数据中的字段包括：Product,Unit,Stock,UnitPrice。\n",
    "    你不用编写代码。\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[\n",
    "    user_proxy, \n",
    "    planner, \n",
    "    engineer,\n",
    "    executor,\n",
    "    ordersAssistant, \n",
    "    productsAssistant], \n",
    "    messages=[], max_round=20)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "获取销量最多商品\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "This model's maximum context length is 8192 tokens. However, you requested 66149 tokens (613 in the messages, 65536 in the completion). Please reduce the length of the messages or completion.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     manager,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#     message=\"\"\"\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# 获取最受欢迎的商品，然后查询该商品的库存量\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# \"\"\",\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#     message=\"\"\"\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# 获取最受欢迎的商品\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# \"\"\",\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m message\u001b[39m=\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m获取销量最多商品\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/00.awesome.Group.Chat.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:162\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[39m# select the next speaker\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39;49mselect_speaker(speaker, \u001b[39mself\u001b[39;49m)\n\u001b[1;32m    163\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mgenerate_reply(sender\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/groupchat.py:91\u001b[0m, in \u001b[0;36mGroupChat.select_speaker\u001b[0;34m(self, last_speaker, selector)\u001b[0m\n\u001b[1;32m     87\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m     88\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGroupChat is underpopulated with \u001b[39m\u001b[39m{\u001b[39;00mn_agents\u001b[39m}\u001b[39;00m\u001b[39m agents. Direct communication would be more efficient.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m selector\u001b[39m.\u001b[39mupdate_system_message(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_speaker_msg(agents))\n\u001b[0;32m---> 91\u001b[0m final, name \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mgenerate_oai_reply(\n\u001b[1;32m     92\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmessages\n\u001b[1;32m     93\u001b[0m     \u001b[39m+\u001b[39;49m [\n\u001b[1;32m     94\u001b[0m         {\n\u001b[1;32m     95\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     96\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mRead the above conversation. Then select the next role from \u001b[39;49m\u001b[39m{\u001b[39;49;00m[agent\u001b[39m.\u001b[39;49mname\u001b[39m \u001b[39;49m\u001b[39mfor\u001b[39;49;00m\u001b[39m \u001b[39;49magent\u001b[39m \u001b[39;49m\u001b[39min\u001b[39;49;00m\u001b[39m \u001b[39;49magents]\u001b[39m}\u001b[39;49;00m\u001b[39m to play. Only return the role.\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     97\u001b[0m         }\n\u001b[1;32m     98\u001b[0m     ]\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m final:\n\u001b[1;32m    101\u001b[0m     \u001b[39m# i = self._random.randint(0, len(self._agent_names) - 1)  # randomly pick an id\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_agent(last_speaker, agents)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    804\u001b[0m         context,\n\u001b[1;32m    805\u001b[0m         use_cache,\n\u001b[1;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    807\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/oai/completion.py:834\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/oai/completion.py:222\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config:\n\u001b[0;32m--> 222\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    223\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m         response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(request_timeout\u001b[39m=\u001b[39mrequest_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/openai/api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[0;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/openai/api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[1;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[1;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[1;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    715\u001b[0m         ),\n\u001b[1;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    717\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/openai/api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[1;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[0;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[1;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mInvalidRequestError\u001b[0m: This model's maximum context length is 8192 tokens. However, you requested 66149 tokens (613 in the messages, 65536 in the completion). Please reduce the length of the messages or completion."
     ]
    }
   ],
   "source": [
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "#     message=\"\"\"\n",
    "# 获取最受欢迎的商品，然后查询该商品的库存量\n",
    "# \"\"\",\n",
    "\n",
    "#     message=\"\"\"\n",
    "# 获取最受欢迎的商品\n",
    "# \"\"\",\n",
    "\n",
    "message=\"\"\"\n",
    "获取销量最多商品\n",
    "\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent instance named \"assistant\"\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    # human_input_mode=\"TERMINATE\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    llm_config=llm_config,\n",
    "    code_execution_config={\"work_dir\": \"00_awesome_poc\"},\n",
    "    system_message=\"\"\"Reply TERMINATE if the task has been solved at full satisfaction.\n",
    "Otherwise, reply CONTINUE, or the reason why the task is not solved yet.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csvFilePath = \"SamplesData/Orders.csv\"\n",
    "\n",
    "df = pd.read_csv(csvFilePath,sep=',')\n",
    "df\n",
    "\n",
    "# df.to_json(orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `AutoGen` 可能能部分解决 `数据不出境` 问题\n",
    "\n",
    "## 营销活动策略-落地\n",
    "1. 读取多个数据表，获得用户、销售、活动洞察\n",
    "   - 提示词 csv: url\n",
    "   - 活动? 时间?\n",
    "1. 总结洞察，形成新活动策略\n",
    "   - 提示词 ????\n",
    "1. 拆解策略（活动名，用户群，活动商品，活动折扣等）\n",
    "   - 提示词 ????\n",
    "   - 定制、干预 ?????\n",
    "\n",
    " \n",
    "\n",
    "   活动（历史）的：活动表[用户群受众、时间、地点、活动商品列表、活动详情、成本]\n",
    "\n",
    "\n",
    "1. 执行策略（生成短信/邮件，生成规则，~~发送~~）\n",
    "   - 仅生成\n",
    "\n",
    "## 运营活动落地：\n",
    "1. 每日按照用户输入活动主题，选择用户群\n",
    "   - 提示词\n",
    "   - 不管分群!!!!\n",
    "1. 选择商品，匹配不同用户群\n",
    "1. 为不同用户群-商品生成活动文案\n",
    "   - 建议不要文案创作\n",
    "   - 仅合并merge(活动表[用户群受众、时间、地点、活动商品列表、活动详情])数据生成文案文本?\n",
    "1. 发送短信\n",
    "   - 建议仅生成别发送了\n",
    "\n",
    "## 我们在研究Autogen的时候，需要想想：\n",
    "- 哪个Agent/角色负责哪一块？\n",
    "  - assistant\n",
    "      `Assist Agent` 设计单一职责:\n",
    "      活动表[用户群受众、时间、地点、活动商品列表、活动详情]\n",
    "      `Assist` 设计策略考虑的因素：预算、成本、收益、回款时长\n",
    "- 有几个user_proxy，分别负责什么？\n",
    "   - `逻辑` 程序退出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://192.168.0.103:5000/SamplesData/Orders.csv?{123}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    # message=\"\"\"Show me the YTD gain of 10 largest technology companies as of today.\"\"\",\n",
    "    # message=\"\"\"展示今天截至目前为止，10家最大的科技公司的年初至今收益。\"\"\",\n",
    "\n",
    "    # succeed\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 然后展示销量最多的商品的名称, 即: \"Product\"\n",
    "    # \"\"\",\n",
    "\n",
    "    # succeed\n",
    "    message=f\"\"\"\n",
    "    请分析订单数据文件: \"{url}\", 获取最受欢迎的商品\n",
    "    \"\"\",\n",
    "\n",
    "    # succeed\n",
    "    #  message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\", 获取最不受欢迎的商品\n",
    "    # \"\"\",\n",
    "\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品该促销?\n",
    "    # \"\"\",\n",
    "\n",
    "    # failed\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品(Product)该停产?\n",
    "    # \"\"\",\n",
    "\n",
    "    # succeed 订单最少的\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品(Product)该打折促销?\n",
    "    # \"\"\",\n",
    "\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个昂贵的商品(Product)该打折促销?\n",
    "    # \"\"\",\n",
    "\n",
    "    # message=f\"\"\"请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个昂贵的商品(Product)该打折促销?\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    # message=\"\"\"Show me the YTD gain of 10 largest technology companies as of today.\"\"\",\n",
    "    # message=\"\"\"展示今天截至目前为止，10家最大的科技公司的年初至今收益。\"\"\",\n",
    "\n",
    "    # succeed\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 然后展示销量最多的商品即: \"Product\"\n",
    "    # \"\"\",\n",
    "\n",
    "    # succeed\n",
    "    #  message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\", 获取最受欢迎的商品\n",
    "    # \"\"\",\n",
    "\n",
    "    # succeed\n",
    "    #  message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\", 获取最不受欢迎的商品\n",
    "    # \"\"\",\n",
    "\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品该促销?\n",
    "    # \"\"\",\n",
    "\n",
    "    # failed\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品(Product)该停产?\n",
    "    # \"\"\",\n",
    "\n",
    "    # succeed 订单最少的\n",
    "    # message=f\"\"\"\n",
    "    # 请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个商品(Product)该打折促销?\n",
    "    # \"\"\",\n",
    "\n",
    "    message=f\"\"\"\n",
    "    请分析订单数据文件: \"{url}\", 请问基于以上数据，能获得哪些洞察，做出什么判断?\n",
    "    \"\"\",\n",
    "\n",
    "    # message=f\"\"\"请分析订单数据文件: \"{url}\"(没有\"quantity\"字段) , 哪个昂贵的商品(Product)该打折促销?\"\"\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen-py311-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
