{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Task Solving with Provided Tools as Functions\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to make function calls with the new feature of OpenAI models (in model version 0613). A specified prompt and function configs must be passed to `AssistantAgent` to initialize the agent. The corresponding functions must be passed to `UserProxyAgent`, which will execute any function calls made by `AssistantAgent`. Besides this requirement of matching descriptions with functions, we recommend checking the system message in the `AssistantAgent` to ensure the instructions align with the function call descriptions.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [mathchat] option since we will import functions from `MathUserProxyAgent`:\n",
    "```bash\n",
    "pip install \"pyautogen[mathchat]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b803c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen~=0.1.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (0.1.14)\n",
      "Requirement already satisfied: diskcache in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (5.6.3)\n",
      "Requirement already satisfied: flaml in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.1.1)\n",
      "Requirement already satisfied: openai<1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.0.0)\n",
      "Requirement already satisfied: termcolor in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.3.0)\n",
      "Requirement already satisfied: pydantic==1.10.9 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (1.10.9)\n",
      "Requirement already satisfied: sympy in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (1.12)\n",
      "Requirement already satisfied: wolframalpha in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pydantic==1.10.9->pyautogen[mathchat]~=0.1.0) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.20 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.8.6)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from flaml->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.26.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from sympy->pyautogen[mathchat]~=0.1.0) (1.3.0)\n",
      "Requirement already satisfied: xmltodict in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (0.13.0)\n",
      "Requirement already satisfied: more-itertools in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (10.1.0)\n",
      "Requirement already satisfied: jaraco.context in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen[mathchat]~=0.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_models`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_models) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints for the provided list of models. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
    "\n",
    "- OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
    "- Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
    "- Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
    "\n",
    "It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base.\n",
    "If you open this notebook in google colab, you can upload your files by clicking the file icon on the left panel and then choosing \"upload file\" icon.\n",
    "\n",
    "The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dca301a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "# The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do.\n",
    "config_list = autogen.config_list_from_models(model_list=[\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"], exclude=\"aoai\")\n",
    "\n",
    "\n",
    "# config_list = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         # \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "#         \"model\": [\"gpt-35-turbo-16k-dep-001\"]\n",
    "#     },\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92fde41f",
   "metadata": {},
   "source": [
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-4\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-3.5-turbo\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-3.5-turbo-16k\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Making Function Calls\n",
    "\n",
    "In this example, we demonstrate function call execution with `AssistantAgent` and `UserProxyAgent`. With the default system prompt of `AssistantAgent`, we allow the LLM assistant to perform tasks with code, and the `UserProxyAgent` would extract code blocks from the LLM response and execute them. With the new \"function_call\" feature, we define functions and specify the description of the function in the OpenAI config for the `AssistantAgent`. Then we register the functions in `UserProxyAgent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb85afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "Draw two agents chatting with each other with an example dialog. Don't add plt.show().\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "disk I/O error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb Cell 8\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m user_proxy\u001b[39m.\u001b[39mregister_function(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     function_map\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m: exec_python,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msh\u001b[39m\u001b[39m\"\u001b[39m: exec_sh,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# start the conversation\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=74'>75</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m     chatbot,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDraw two agents chatting with each other with an example dialog. Don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt add plt.show().\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:781\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 781\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    783\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/oai/completion.py:803\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    801\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 803\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    804\u001b[0m         context,\n\u001b[1;32m    805\u001b[0m         use_cache,\n\u001b[1;32m    806\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    807\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    808\u001b[0m     )\n\u001b[1;32m    809\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    810\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/autogen/oai/completion.py:832\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    831\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(params\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m--> 832\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39;49mCache(\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    833\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[1;32m    834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/diskcache/core.py:478\u001b[0m, in \u001b[0;36mCache.__init__\u001b[0;34m(self, directory, timeout, disk, **settings)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(sets\u001b[39m.\u001b[39mitems()):\n\u001b[1;32m    477\u001b[0m     \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39msqlite_\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset(key, value, update\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    480\u001b[0m sql(\n\u001b[1;32m    481\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCREATE TABLE IF NOT EXISTS Settings (\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    482\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m key TEXT NOT NULL UNIQUE,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    483\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m value)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    484\u001b[0m )\n\u001b[1;32m    486\u001b[0m \u001b[39m# Setup Disk object (must happen after settings initialized).\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/diskcache/core.py:2431\u001b[0m, in \u001b[0;36mCache.reset\u001b[0;34m(self, key, value, update)\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2430\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2431\u001b[0m         ((old_value,),) \u001b[39m=\u001b[39m sql(\n\u001b[1;32m   2432\u001b[0m             \u001b[39m'\u001b[39;49m\u001b[39mPRAGMA \u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m (pragma)\n\u001b[1;32m   2433\u001b[0m         )\u001b[39m.\u001b[39mfetchall()\n\u001b[1;32m   2434\u001b[0m         update \u001b[39m=\u001b[39m old_value \u001b[39m!=\u001b[39m value\n\u001b[1;32m   2435\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: disk I/O error"
     ]
    }
   ],
   "source": [
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"python\",\n",
    "            \"description\": \"run cell in ipython and return the execution result.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"cell\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Valid Python cell to execute.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"cell\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sh\",\n",
    "            \"description\": \"run a shell script and return the execution result.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"script\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Valid shell script to execute.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"script\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "}\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"work_dir\": \"coding\"},\n",
    ")\n",
    "\n",
    "# define functions according to the function description\n",
    "from IPython import get_ipython\n",
    "\n",
    "def exec_python(cell):\n",
    "    ipython = get_ipython()\n",
    "    result = ipython.run_cell(cell)\n",
    "    log = str(result.result)\n",
    "    if result.error_before_exec is not None:\n",
    "        log += f\"\\n{result.error_before_exec}\"\n",
    "    if result.error_in_exec is not None:\n",
    "        log += f\"\\n{result.error_in_exec}\"\n",
    "    return log\n",
    "\n",
    "def exec_sh(script):\n",
    "    return user_proxy.execute_code_blocks([(\"sh\", script)])\n",
    "\n",
    "# register the functions\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"python\": exec_python,\n",
    "        \"sh\": exec_sh,\n",
    "    }\n",
    ")\n",
    "\n",
    "# start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"Draw two agents chatting with each other with an example dialog. Don't add plt.show().\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9531d55",
   "metadata": {},
   "source": [
    "## Another example with Wolfram Alpha API\n",
    "\n",
    "We give another example of querying Wolfram Alpha API to solve math problem. We use the predefined function `MathUserProxyAgent().execute_one_wolfram_query` as the function to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a917492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "Problem: Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_wolfram *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"solve (2x+10)(x+3)<(3x+9)(x+8) for x\"\n",
      "}\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_wolfram...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_wolfram\" *****\u001b[0m\n",
      "('Assumption: solve (2 x + 10) (x + 3)<(3 x + 9) (x + 8) for x \\nAnswer: ans 0: x<-14\\nans 1: x>-3\\n', True)\n",
      "\u001b[32m**********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "The solution to the inequality $(2x+10)(x+3)<(3x+9)(x+8)$ is $x \\in (-\\infty, -14) \\cup (-3, +\\infty)$. TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen.agentchat.contrib.math_user_proxy_agent import MathUserProxyAgent\n",
    "\n",
    "# you need to provide a wolfram alpha app id to run this example\n",
    "if not os.environ.get(\"WOLFRAM_ALPHA_APPID\"):\n",
    "    os.environ[\"WOLFRAM_ALPHA_APPID\"] = open(\"wolfram.txt\").read().strip()\n",
    "\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-4-0613\",\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"query_wolfram\",\n",
    "            \"description\": \"Return the API query result from the Wolfram Alpha. the return is a tuple of (result, is_success).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Wolfram Alpha code to be executed.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"Only use the functions you have been provided with. Do not ask the user to perform other actions than executing the functions. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# the key in `function_map` should match the function name in \"functions\" above\n",
    "# we register a class instance method directly\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    function_map={\"query_wolfram\": MathUserProxyAgent().execute_one_wolfram_query},\n",
    ")\n",
    "\n",
    "# start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"Problem: Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
