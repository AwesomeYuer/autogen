{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae1f50ec",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a71fa36",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Task Solving with Provided Tools as Functions\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool, or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation. Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "In this notebook, we demonstrate how to use `AssistantAgent` and `UserProxyAgent` to make function calls with the new feature of OpenAI models (in model version 0613). A specified prompt and function configs must be passed to `AssistantAgent` to initialize the agent. The corresponding functions must be passed to `UserProxyAgent`, which will execute any function calls made by `AssistantAgent`. Besides this requirement of matching descriptions with functions, we recommend checking the system message in the `AssistantAgent` to ensure the instructions align with the function call descriptions.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install the [mathchat] option since we will import functions from `MathUserProxyAgent`:\n",
    "```bash\n",
    "pip install \"pyautogen[mathchat]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b803c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyautogen~=0.1.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (0.1.14)\n",
      "Requirement already satisfied: diskcache in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (5.6.3)\n",
      "Requirement already satisfied: flaml in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.1.1)\n",
      "Requirement already satisfied: openai<1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.0.0)\n",
      "Requirement already satisfied: termcolor in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.3.0)\n",
      "Requirement already satisfied: pydantic==1.10.9 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (1.10.9)\n",
      "Requirement already satisfied: sympy in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (1.12)\n",
      "Requirement already satisfied: wolframalpha in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pyautogen[mathchat]~=0.1.0) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from pydantic==1.10.9->pyautogen[mathchat]~=0.1.0) (4.6.3)\n",
      "Requirement already satisfied: requests>=2.20 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.8.6)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from flaml->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.26.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from sympy->pyautogen[mathchat]~=0.1.0) (1.3.0)\n",
      "Requirement already satisfied: xmltodict in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (0.13.0)\n",
      "Requirement already satisfied: more-itertools in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (10.1.0)\n",
      "Requirement already satisfied: jaraco.context in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from wolframalpha->pyautogen[mathchat]~=0.1.0) (4.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from requests>=2.20->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/awesomeyuer/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages (from aiohttp->openai<1->pyautogen~=0.1.0->pyautogen[mathchat]~=0.1.0) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"pyautogen[mathchat]~=0.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ebd2397",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_models`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_models) function tries to create a list of configurations using Azure OpenAI endpoints and OpenAI endpoints for the provided list of models. It assumes the api keys and api bases are stored in the corresponding environment variables or local txt files:\n",
    "\n",
    "- OpenAI API key: os.environ[\"OPENAI_API_KEY\"] or `openai_api_key_file=\"key_openai.txt\"`.\n",
    "- Azure OpenAI API key: os.environ[\"AZURE_OPENAI_API_KEY\"] or `aoai_api_key_file=\"key_aoai.txt\"`. Multiple keys can be stored, one per line.\n",
    "- Azure OpenAI API base: os.environ[\"AZURE_OPENAI_API_BASE\"] or `aoai_api_base_file=\"base_aoai.txt\"`. Multiple bases can be stored, one per line.\n",
    "\n",
    "It's OK to have only the OpenAI API key, or only the Azure OpenAI API key + base.\n",
    "If you open this notebook in google colab, you can upload your files by clicking the file icon on the left panel and then choosing \"upload file\" icon.\n",
    "\n",
    "The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dca301a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "# The following code excludes Azure OpenAI endpoints from the config list because some endpoints don't support functions yet. Remove the `exclude` argument if they do.\n",
    "# config_list = autogen.config_list_from_models(model_list=[\"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"], exclude=\"aoai\")\n",
    "\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        # \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "        \"model\": [\"gpt-4\"]\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# config_list = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         # \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "#         \"model\": [\"gpt-35-turbo-16k-dep-001\"]\n",
    "#     },\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92fde41f",
   "metadata": {},
   "source": [
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-4\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-3.5-turbo\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo-16k',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },  # OpenAI API endpoint for gpt-3.5-turbo-16k\n",
    "]\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b9526e7",
   "metadata": {},
   "source": [
    "## Making Function Calls\n",
    "\n",
    "In this example, we demonstrate function call execution with `AssistantAgent` and `UserProxyAgent`. With the default system prompt of `AssistantAgent`, we allow the LLM assistant to perform tasks with code, and the `UserProxyAgent` would extract code blocks from the LLM response and execute them. With the new \"function_call\" feature, we define functions and specify the description of the function in the OpenAI config for the `AssistantAgent`. Then we register the functions in `UserProxyAgent`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb85afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "获取主机ip和用户名称\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: sh *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"script\": \"hostname -I && whoami\"\n",
      "}\n",
      "\u001b[32m***************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION sh...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"sh\" *****\u001b[0m\n",
      "(1, \"\\nhostname: unrecognized option: I\\nBusyBox v1.36.1 (2023-07-27 17:12:24 UTC) multi-call binary.\\n\\nUsage: hostname [-sidf] [HOSTNAME | -F FILE]\\n\\nShow or set hostname or DNS domain name\\n\\n\\t-s\\tShort\\n\\t-i\\tAddresses for the hostname\\n\\t-d\\tDNS domain name\\n\\t-f\\tFully qualified domain name\\n\\t-F FILE\\tUse FILE's content as hostname\\n\")\n",
      "\u001b[32m***********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "Sorry for the inconvenience, it seems there was an error running the previous script. Let's try another method.\n",
      "\u001b[32m***** Suggested function Call: sh *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"script\": \"hostname && whoami\"\n",
      "}\n",
      "\u001b[32m***************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION sh...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"sh\" *****\u001b[0m\n",
      "(0, '\\n1906e8940f8b\\nroot\\n')\n",
      "\u001b[32m***********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "The hostname (which may also be the IP address) of the machine is '1906e8940f8b', and the current user is 'root'.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"python\",\n",
    "            \"description\": \"run cell in ipython and return the execution result.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"cell\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Valid Python cell to execute.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"cell\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"sh\",\n",
    "            \"description\": \"run a shell script and return the execution result.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"script\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Valid shell script to execute.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"script\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "}\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"For coding tasks, only use the functions you have been provided with. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    code_execution_config={\"work_dir\": \"coding\"},\n",
    ")\n",
    "\n",
    "# define functions according to the function description\n",
    "from IPython import get_ipython\n",
    "\n",
    "def exec_python(cell):\n",
    "    ipython = get_ipython()\n",
    "    result = ipython.run_cell(cell)\n",
    "    log = str(result.result)\n",
    "    if result.error_before_exec is not None:\n",
    "        log += f\"\\n{result.error_before_exec}\"\n",
    "    if result.error_in_exec is not None:\n",
    "        log += f\"\\n{result.error_in_exec}\"\n",
    "    return log\n",
    "\n",
    "def exec_sh(script):\n",
    "    return user_proxy.execute_code_blocks([(\"sh\", script)])\n",
    "\n",
    "# register the functions\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"python\": exec_python,\n",
    "        \"sh\": exec_sh,\n",
    "    }\n",
    ")\n",
    "\n",
    "# start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    # message=\"Draw two agents chatting with each other with an example dialog. Don't add plt.show().\",\n",
    "    # message=\"获取主机ip和用户名称\",\n",
    "    message=\"获取 ifconfig 输出结果\",\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9531d55",
   "metadata": {},
   "source": [
    "## Another example with Wolfram Alpha API\n",
    "\n",
    "We give another example of querying Wolfram Alpha API to solve math problem. We use the predefined function `MathUserProxyAgent().execute_one_wolfram_query` as the function to be called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a917492",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wolfram.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# you need to provide a wolfram alpha app id to run this example\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mWOLFRAM_ALPHA_APPID\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     os\u001b[39m.\u001b[39menviron[\u001b[39m\"\u001b[39m\u001b[39mWOLFRAM_ALPHA_APPID\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mwolfram.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m llm_config \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mgpt-4-0613\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfunctions\u001b[39m\u001b[39m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconfig_list\u001b[39m\u001b[39m\"\u001b[39m: config_list,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m chatbot \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchatbot\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOnly use the functions you have been provided with. Do not ask the user to perform other actions than executing the functions. Reply TERMINATE when the task is done.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     llm_config\u001b[39m=\u001b[39mllm_config,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04.2-lts-2023-06-27/mnt/d/MyGitHub/autogen/notebook/agentchat_function_call.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen-py311-env/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wolfram.txt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autogen.agentchat.contrib.math_user_proxy_agent import MathUserProxyAgent\n",
    "\n",
    "# you need to provide a wolfram alpha app id to run this example\n",
    "if not os.environ.get(\"WOLFRAM_ALPHA_APPID\"):\n",
    "    os.environ[\"WOLFRAM_ALPHA_APPID\"] = open(\"wolfram.txt\").read().strip()\n",
    "\n",
    "llm_config = {\n",
    "    \"model\": \"gpt-4-0613\",\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"query_wolfram\",\n",
    "            \"description\": \"Return the API query result from the Wolfram Alpha. the return is a tuple of (result, is_success).\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The Wolfram Alpha code to be executed.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "chatbot = autogen.AssistantAgent(\n",
    "    name=\"chatbot\",\n",
    "    system_message=\"Only use the functions you have been provided with. Do not ask the user to perform other actions than executing the functions. Reply TERMINATE when the task is done.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# the key in `function_map` should match the function name in \"functions\" above\n",
    "# we register a class instance method directly\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    \"user_proxy\",\n",
    "    max_consecutive_auto_reply=2,\n",
    "    human_input_mode=\"NEVER\",\n",
    "    function_map={\"query_wolfram\": MathUserProxyAgent().execute_one_wolfram_query},\n",
    ")\n",
    "\n",
    "# start the conversation\n",
    "user_proxy.initiate_chat(\n",
    "    chatbot,\n",
    "    message=\"Problem: Find all $x$ that satisfy the inequality $(2x+10)(x+3)<(3x+9)(x+8)$. Express your answer in interval notation.\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
